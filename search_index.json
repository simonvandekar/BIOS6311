[["index.html", "BIOS6311: Principles of Modern Biostatistics Course information MIDTERM NOVEMBER 9TH 2023 Syllabus 0.1 Labs 0.2 Homework", " BIOS6311: Principles of Modern Biostatistics Simon Vandekar 2023-08-19 Course information This document includes all the course notes for Vanderbilt Biostatistics BIOS6311 Principles of Modern Biostatistics. MIDTERM NOVEMBER 9TH 2023 From that Rice book above here are some good practice problems: Section 4.7: 2, 16, 44, 47, 50, 54, 68 (not really that helpful) Section 5.4: 11 (might be helpful), 17 (the equality should be approximate equality), Section 7.7: 3, 8, 10, 16, 17, 18, 21 Syllabus Schedule: Tuesdays/Thursdays 10:30am-12pm (in-person) Lab: Tuesdays 4pm-5pm (in-person) Instructor: Simon Vandekar (he/him/his) TA: Yeji Kuo Office hours: by appointment Email: simon.vandekar@vumc.org. I do not check my vanderbilt.edu email address, regularly. 0.0.1 Overview Biostatistics (and statistics) is a framework for learning about the world that is based on the theory of probability. The goal of this class is to teach the concepts of biostatistics through basic examples that form the content of the class using software tools for reproducible statistical research in R. The concepts are the basis for modern statistical research – i.e. the foundational concepts you learn here can be used to evaluate modern statistical methods. The objective content will cover basic statistical objects (e.g. means), with as much focus as possible on rigorous modern methods. My interests are in medical imaging in the fields of psychology, psychiatry, and cellular biology, so I will use examples from those fields, hoping to pique your interest in those research areas. 0.0.2 Goal Upon completion of this course, you should be able to evaluate statistical methods based on their operating characteristics. While the concepts for methods evaluation we will learn are more general, they will be taught in the context of one and two-sample statistical methods within Frequentist, Likelihood, and Bayesian philosophies with as much focus as possible on rigorous modern methods. If the course is effective you will Strengthen mathematical tools for understanding and evaluating statistical methods Learn to analyze a dataset using basic statistical inference tools (e.g. one sample/two sample tests) Learn to program statistical evaluation tools in R Learn to use the goals and concepts of the three statistical philosophies Evaluate statistical methods using simulations and bootstrapping Understand how statisticians choose and design statistical methods 0.0.3 Datasets Most of the course will be taught through applications with a dataset. I will make a couple datasets available for you to apply your statistical methods and evaluation tools. 0.0.4 Homework Homework will be your primary grade for the course. It will also be the way that you will learn software tools through hands on experience: we will plan for you to submit most of homework assignments using Rmarkdown. 0.0.5 Exams and Quizzes Previously, I just had fill-in-the-blank quizzes and a final exam. This time we will have an open-answer midterm and final to help prepare you for the first qualifying exam. 0.0.6 Class This will be an in-person class, but I might record the lectures. We will meet in-person twice a week (Tues/Thurs 10:30am-12pm). In class meetings we’ll be working to solve math and programming problems together that will help you do your homework. 0.0.7 Lab There will be as many lab activities as I have time to plan. Otherwise that time will be used to help debug code, catch up on missed classes, or learn the materials and work together or in groups on the homework. Background necessary for the course: The official background for the course is Calculus 1. I will teach as if you have taken introductory probability, such as “A first course in probability,” by Ross. I assume novice level literacy in R statistical programming, but experience in another language will be helpful to learning R (in particular Matlab and Python). I will use some linear algebra if the class as a whole seems to have sufficient background in this area. 0.0.8 Text I’ve based the class off of previous versions by Robert Greevy and “Mathematical Statistics and Data Analysis,” by Rice. I will not explicitly use this book in the class, but it is probably the most complete reference for the material. I will point to other references as they come up. 0.0.9 Class resources Previously I’ve done everything in Brightspace, but to make it more accessible, I am going to host it through a course webpage instead. Brightspace - For class communication, homework submission and grading. Pumble - For class communication. Box - For sharing datasets and files. 0.0.10 Diversity and inclusion: Creativity is critical to research in statistics. In statistics, creativity comes in the form of questions (e.g. what happens if this model is incorrect?; when I condition on a subset of the data, what things are still random?) Diversity is at the heart of creativity, because our perspectives shape how we see the world. By working together to consider other’s perspectives, we will learn more about the problems we discuss. For this reason, please interact with each other in a spirit of curiosity and empathy (not just about the class content, but as you learn about the people you will go through grad school with). I expect this class to be a welcoming environment for all students, regardless of background, skin color, gender identity, orientation, spiritual belief or any other personal feature. 0.0.11 Course outline I might adapt course content based on the mine or the classes interests. This is generally how it has looked: Probability tools and R Law of Large Numbers and Central Limit Theorem Confidence intervals for a single mean Z intervals T intervals CIs for binomial proportion Hypothesis testing Paired means Two means Welch’s correction Likelihood theory Features of the likelihood function Bayesian statistics Binomial proportion Normal mean Contingency tables and tests for two binomial means (might switch this for effect sizes) Resampling and Nonparametric methods Overview of regression 0.1 Labs All the shared files are here. Here is a list of links to the lab activities. Week 1 0.2 Homework Here is a list of links for the homework. Homework 1 Homework 2 Homework 3 "],["glossaryjargon.html", "Glossary/Jargon", " Glossary/Jargon Reference distribution - the standardized distribution used to compute probabilities. Examples include the Normal distribution, Chi-squared distribution, T distribution, and F distribution. Sampling distribution - the true distribution of a statistic drawn from data randomly sampled from a distribution. For a test statistic, it may not be the same as the reference distribution if we are using approximations or assumptions are violated. Statistic - a random variable computed as a function of a random sample. Test statistic - the standardized value, which is assumed to follow the reference distribution. One/two tailed hypothesis Paired/unpaired test One/two sample test Skew Type 1 error rate - the probability of falsely rejecting the null when it is true. Type 2 error rate - the probability of falsely retaining the null when it is false. Power - the probability of correctly rejecting the null. Wald statistic - A statistic derived as the estimator minus the parameter divided by the variance of the estimator times square root of \\(n\\), \\(\\frac{\\sqrt{n}(\\hat\\mu - \\mu)}{\\text{Var}(\\sqrt{n} \\hat\\mu)}\\). Parameter - an unknown population value. Estimator - a statistic used to estimate a parameter. Estimate - a function of an observed sample used to estimate a parameter (not random). Coverage - The proportion of the time that an interval captures the true value of the parameter. Width/Length - The expected width/length of a confidence interval is the distance between the upper and lower bounds. Consistent estimator - an estimator that converges to the target parameter as the sample size gets bigger. Standard error - the standard deviation of a statistic. "],["how-to-follow-along-in-this-class.html", "How to follow along in this class 0.3 Class notes 0.4 Labs and homeworks", " How to follow along in this class 0.3 Class notes I will use the whiteboard and Rstudio for teaching. The notes and code will be based of of what is available in this document. 0.3.1 Written notes Because I will be writing on the whiteboard for written notes, you can Follow along with your own notebook. Print the notes and add your own. To print the notes, Push the lines in the hover menu at the top to close the sidebar and then print to pdf or paper in your browser. Please print two-sided. Save as a PDF and open in Notes app on a tablet where you can annotate the file. 0.3.2 Code notes The code examples in the course may be fully written in the notes, or they may be barebones that we complete together in class. I will try to update the code in this document, so that the code is completed here after we have written it in class. To follow along with R code you can Create an Rmarkdown file with a section for each chunk of example code we run and complete the code as we complete it in class. Write the code in separate .R scripts organized in a file for the course. 0.4 Labs and homeworks This document and all the files in the course are compiled using Rmarkdown. For completing the labs and homework assignments, I’m assuming that you will download the files and type your answers into the documents. There will be a bit of a learning curve, so it’s best to start working with it on the first assignments, so that you don’t feel too overwhelmed trying to learn Rmarkdown and the course content. Check out the Week 1 lab assignment as an example of the folder structure I will use for homework and labs. You can download the whole folder and work from there. There are the following files in the folder that you need to pay attention to: lab1_2.Rproj - this is R project file that opens up a saved workspace in Rstudio. lab1_2.Rmd - this is the Rmarkdown file that you can edit to complete your answers. lab1_2.html - this is the output from the Rmarkdown file. When you add your answers and hit the Knit button in Rstudio it will compile this file. You can change the format between html/pdf/docx. The other files are intermediate files created after you push the Knit button. "],["probability.html", "1 Probability 1.1 Probability review 1.2 CDFs, PDFs, quantile functions 1.3 Multivariate random variables 1.4 Parameters, estimates, and estimators", " 1 Probability 1.1 Probability review 1.1.1 Objectives In this meeting we will: Introductions Review some probability notation (e.g. PMF, Expectations) Use Bernoulli random variables to study proportions Define Estimators and learn to compute values assess estimators 1.1.2 Administrative stuff Introductions How did you get interested in statistics? What do you do for fun? Syllabus 1.1.3 Types of data Continuous Categorical Ordinal Examples: Brain volume Diagnosis Symptom rating scale (1-7) Coin flip Proportion of heads in \\(N\\) flips Proportion of time spent sleeping each week 1.1.4 Random variables Random variables are the key player in statistics and are often used to describe the process by which data arise. 1.1.4.1 Probability notation Blackboard P is probability, \\(\\mathbb{P}(X=x)\\), means the probability the random variable \\(X\\) is equal to the nonrandom value \\(x\\). \\(\\sim\\) “is distributed as” Probability mass function (PMF; loose definition is it assigns probabilities to values for a given random variable) Continuous/Discrete random variables A random variable does not have a value in itself… We don’t usually talk about \\(X=0.5\\), but \\(\\mathbb{P}(X=0.5)\\). 1.1.4.1.1 Bernoulli example Something that takes only two values can be described with a Bernoulli random variable. Coin flip Diagnosis For \\(X\\sim \\text{Be}(\\pi)\\): X is distributed as Bernoulli, with parameter \\(\\pi \\in (0,1)\\), e.g. \\(\\pi=0.25\\). \\(\\mathbb{P}(X = x) = \\pi^x(1-\\pi)^{(1-x)}\\). That is the probability mass function (PMF) of a Bernoulli random variable Common notation is \\(f_X(x)\\) is the PMF of the random variable \\(X\\). In this case \\(f_X(x):=\\pi^x(1-\\pi)^{(1-x)}\\). \\(:=\\) notation means “is defined by.” 1.1.4.2 Probability axioms We have an intuitive understanding of the basic axioms of probability. An event \\(E\\) is something that can occur, e.g. head or tails \\(\\{0,1\\}\\). Let \\(\\Omega = {0,1}\\) be union of all possible events The (Kolmogorov) axioms are \\(\\mathbb{P}(X\\in E) \\ge 0\\) – probability is positive. \\(\\mathbb{P}(X\\in \\Omega) = 1\\) – probabilities sum to 1. For disjoint sets \\(E_1 \\cap E_2 = \\varnothing\\), \\(\\mathbb{P}(E_1 \\cup E_2) = \\mathbb{P}(E_1) + \\mathbb{P}(E_2)\\). 1.1.4.3 Why use probabilities for data? STOPPED HERE Intuitive and communicable explanation? Table below from Chess in the air Risk of dying 1.1.4.4 Multinomial example A die is an example of a multinomial distribution Code die = data.frame(&#39;X&#39;=1:6, &#39;P(X)&#39;=paste0(&#39;$\\\\pi_&#39;, 1:6, &#39;$&#39;), check.names = FALSE) knitr::kable(die) X P(X) 1 \\(\\pi_1\\) 2 \\(\\pi_2\\) 3 \\(\\pi_3\\) 4 \\(\\pi_4\\) 5 \\(\\pi_5\\) 6 \\(\\pi_6\\) Can also think about statements like \\(\\mathbb{P}(X\\le x)\\) This is a sum over probabilities corresponding to possible values of \\(X\\). Missing probabilities are called parameters e.g. x = 2 \\[ \\sum_{i=1}^2 \\mathbb{P}(X=i) = 1/3 \\] 1.1.4.5 Wordle Multinomial example Multinomial can be used for distributions of other things E.g. Wordle scores. The probability of getting the Wordle in a certain number of guesses. Also 6 (or one more) possible values. Same family of distribution as the die, except with different parameters \\(Y=\\)Number of tries \\(P(Y)\\) 1 0 2 0.03 3 0.28 4 0.40 5 0.18 6 0.11 7 ?? Code wordle = c(0, 2, 17, 25, 11, 7) #wordle/sum(wordle) #round(wordle/sum(wordle), 2) Wordle results 1.1.4.6 Normal example The normal density has two parameters \\(\\mu, \\sigma\\) is \\[ \\mathbb{P}(X\\le x) = \\int_{-\\infty}^x (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2}(z-\\mu)^2 \\right\\}dz \\] Probabilities of sets \\(\\mathbb{P}(X \\in E)\\), where \\(E = (-\\infty, -1.96) \\cup (1.96, \\infty)\\). Probability \\(X=x\\)? Examples: Multinomial – after a rolling a die, it has to land on one of them. Normal, \\(E=(-\\infty,\\infty)\\). If not disjoint, then adding some probabilities twice. Multinomial \\(\\{1,2,3 \\}, \\{4,3\\}\\) 1.2 CDFs, PDFs, quantile functions 1.2.1 Objectives Introduce CDFs, PDFs, and quantile functions Introduce Expectations (means) 1.2.2 PDFs and CDFs (and quantile functions) 1.2.2.1 PDFs The PDF (probability density function) is the derivative of the CDF and often denoted with a lower case letter \\(f(x)\\). For discrete random variables the PDF is call the PMF (probability mass function). Figure 1.1: Corresponding density functions. 1.2.2.2 PDFs continued The PDF can conceptually be thought of as \\(\\mathbb{P}(X=x)\\), for PMFs, that’s exactly what it is. But, for continuous random variables the probability they take any value is equal to zero. These PDFs are functions with parameters: \\[ \\begin{align*} \\text{Normal: }&amp; f(x; \\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2 \\right\\} \\\\ \\text{Gamma: }&amp; f(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x} \\\\ \\text{Poisson: }&amp; f(x; \\lambda) = \\frac{\\lambda^k\\ e^{-\\lambda}}{k!} \\\\ \\end{align*} \\] The CDF (below) and PDF are useful because we never know what values the random variable will take, so we can integrate the PDF or use the CDF to figure out where they are most likely to fall. 1.2.2.3 CDFs The cumulative distribution function (CDF), \\(F(x)\\), for a random variable \\(X\\) is a function that satisfies \\[ F(x) = \\mathbb{P}(X \\le x). \\] They are usually functions of parameters. Here are some examples. Figure 1.2: Some distribution functions. 1.2.2.3.1 Human Connectome Project Brain volume example A brain. Human Connectome Project data Study designed to understand how regions of the brain are interconnected Code hcp = read.csv(&#39;../datasets/hcp/hcp.csv&#39;) mu = mean(hcp$FS_TotCort_GM_Vol, na.rm=T) sigma = sd(hcp$FS_TotCort_GM_Vol, na.rm=T) mu ## [1] 510019.7 Code sigma ## [1] 53940.92 Code x = seq(mu - 3*sigma, mu + 3*sigma, length.out=1000) fOfX = dnorm(x, mean = mu, sd=sigma) plot(x, fOfX, type=&#39;l&#39;, xlab=&#39;Brain volume&#39;, ylab=&#39;f(Brain Volume)&#39;) Code hist(hcp$FS_TotCort_GM_Vol, xlab=&#39;Brain Volume&#39;, xlim=c(mu - 3*sigma, mu + 3*sigma)) For discrete random variables the derivative of the CDF does not exist because it is a step function, but the probability mass function is the amount the CDF jumps up at that location, heuristically we can define it as \\[ f(x) = F(x+\\Delta x) - F(x), \\] for an infinitesimal value \\(\\Delta x\\). 1.2.2.3.2 Wordle CDF example \\(Y=\\)Number of tries \\(P(Y|\\text{Simon})\\) \\(P(Y|\\text{Lillie})\\) 1 0 0 2 0.03 0.05 3 0.28 0.24 4 0.40 0.44 5 0.18 0.19 6 0.11 0.08 Code wordle1 = c(0, 2, 17, 25, 11, 7) wordle2 = c(0, 2, 9, 16, 7, 3) wordle1 = cumsum(wordle1/sum(wordle1)) wordle2 = cumsum(wordle2/sum(wordle2)) x = 0:7 plot(x, c(0,wordle1, 1), main=&#39;Wordle CDFs&#39;, type=&#39;s&#39;, ylab=&#39;P(X&lt;=x)&#39;) points(x, c(0,wordle2, 1), type=&#39;s&#39;, col=&#39;red&#39;) 1.2.2.3.3 Human Connectome Project CDF example (continued) We can compute probabilities that people lie within a given region using the CDF. Code x = seq(mu - 3*sigma, mu + 3*sigma, length.out=1000) fOfX = dnorm(x, mean = mu, sd=sigma) plot(x, fOfX, type=&#39;l&#39;, xlab=&#39;Brain volume&#39;, ylab=&#39;f(Brain Volume)&#39;) Probability someone is between \\(450,000\\mathrm{mm}^3\\) and \\(450,000\\mathrm{mm}^3\\) Probability someone is less than \\(400,000\\mathrm{mm}^3\\). probability someone is greater than\\(500,000\\mathrm{mm}^3\\) Code # center pnorm(550000, mean=mu, sd=sigma) - pnorm(450000, mean=mu, sd=sigma) ## [1] 0.6377898 Code # lower tail pnorm(400000, mean=mu, sd=sigma) ## [1] 0.0206934 Code # upper tail 1-pnorm(600000, mean=mu, sd=sigma) ## [1] 0.0476453 1.2.2.4 Quantile functions The quantile function is the inverse of the CDF. For a given probability \\(p\\), it spits out a value \\(Q(p)\\) such that that \\(F(Q(p)) = p\\). The interpretation is that for a given probability \\(p\\), There’s a \\(p\\) percent probability that the random variable will be below \\(Q(p)\\). Sometimes, it’s possible to find the quantile function explicitly, but many times it isn’t. Also can call these “percentiles” if you multiply them by 100. 1.2.2.4.1 HCP example 5% of people have a brain volume larger than what value? What is the middle value (median; .5 quantile) of brain volumes? What is interquartile range (difference between .75 quantile and .25 quantile)? Code # 95 percentile of brain volume qnorm(1-0.05, mean=mu, sd=sigma) ## [1] 598744.6 Code # Median qnorm(0.5, mean=mu, sd=sigma) ## [1] 510019.7 Code # IQR qnorm(0.75, mean=mu, sd=sigma) - qnorm(0.25, mean=mu, sd=sigma) ## [1] 72765.19 1.2.3 Expectations 1.2.3.1 Overview Many things we care about about a random variable are expectations. The expectation is an “operator” on a random variable, meaning it takes function of a random variable \\(g(X)\\) for a (somewhat) arbitrary function \\(g\\) and is defined by \\[ \\mathbb{E} g(X) = \\int_{\\mathcal{X}} g(x) p(x) dx, \\] where \\(p(x)\\) is the density function of \\(X\\) and the \\(\\mathcal{X}\\) subscript is to denote that we are integrating over the domain of values \\(X\\) can take. 1.2.3.2 Great expectations The mean is the most common expectation \\[ \\mathbb{E} X = \\int_{\\mathcal{X}} x p(x) dx. \\] The integral notation is in the sense of “real analysis” type integrals that can refer to sums or integrals. This is to emphasize that the definition is the same with continuous or discrete random variables. Another great expectation is the variance \\[ \\text{Var}(X) = \\mathbb{E} (X - \\mathbb{E}X)^2 = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\] Properties of the variance are homework questions. In general, \\[ \\mathbb{E}X^k \\] is call the \\(k\\)th moment of \\(X\\). 1.2.3.3 Properties of expectations \\(\\mathbb{E}a X + b = a \\mathbb{E}X + b\\) \\(\\mathbb{E}\\{ \\mathbb{E}X\\} = \\mathbb{E}X\\) (\\(\\mathbb{E}X\\) is a constant) 1.2.3.4 Expected value for Bernoulli For the Bernoulli \\(X\\sim \\text{Be}(p)\\) the expectation is \\[ \\mathbb{E}X = \\sum_{i=0}^1 x_i f_X(x_i) = 1 \\times p + 0 \\times (1-p) = p \\] 1.2.3.5 Expected value for Multinomial \\(Y\\) \\(P(Y)\\) \\(P(Y)\\) \\(P(Y)\\) 1 1/4 .1 .7 2 1/4 .2 .1 3 1/4 .3 .1 4 1/4 .4 .1 1.2.3.6 Expected value for Wordle score \\(Y=\\)Number of tries \\(P(Y)\\) 1 0 2 0.03 3 0.28 4 0.40 5 0.18 6 0.11 Code wordle = c(0, 2, 17, 25, 11, 7) #wordle/sum(wordle) #round(wordle/sum(wordle), 2) Wordle results 1.2.3.7 Expected value for Poisson random variable Poisson random variables are useful for modeling skewed counts. E.g. number of drinks of alcohol in a week. E.g. score on a depression inventory or other psychology/scale. \\[ f(x; \\lambda) = \\frac{\\lambda^x\\ e^{-\\lambda}}{x!} \\] Expectation and Variance of Poisson distribution Code x = seq(0, 15) plot(x, dpois(x, lambda=5), main=&#39;Poisson(5)/Poisson(0.5)&#39;, type=&#39;p&#39;, ylab=&#39;P(X=x)&#39;, ylim=c(0,1)) points(x, dpois(x, lambda=0.5), type=&#39;p&#39;, col=&#39;red&#39;) 1.2.3.8 Expected value for standard Normal distribution Start with the standardized normal distribution \\[\\begin{align*} \\mathbb{E}X &amp; = \\int_{-\\infty}^\\infty x \\left(2\\pi \\right)^{-1/2} \\exp\\left\\{-\\frac{1}{2} x^2 \\right\\}\\\\ &amp; = \\left(2\\pi \\right)^{-1/2} \\int_{0}^\\infty \\exp\\{-u\\} du - \\left(2\\pi \\right)^{-1/2} \\int_{0}^\\infty \\exp\\{-u\\} du \\end{align*}\\] where \\(u = x^2/2\\). This equals zero. 1.2.3.9 Expected value for Normal distribution Start with the standardized normal distribution and use change of variables. For \\(X\\sim N(0,1)\\), let \\(Y = X\\sigma + \\mu\\). What is \\(\\mathbb{E}Y\\)? Then \\[\\begin{align*} \\mathbb{P}(Y&lt;z) = \\mathbb{P}((X\\sigma+\\mu)&lt;z) = \\mathbb{P}(X&lt; (z - \\mu)/\\sigma) \\\\ = \\int_{-\\infty}^{(z-\\mu)/\\sigma} \\left(2\\pi \\right)^{-1/2} \\exp\\left\\{-\\frac{1}{2} x^2 \\right\\} dx\\\\ = \\int_{-\\infty}^{y} \\left(2\\pi \\sigma^2\\right)^{-1/2} \\exp\\left\\{-\\frac{1}{2\\sigma^2} (w-\\mu)^2 \\right\\} dw \\end{align*}\\] This is the CDF a \\(N(\\mu, \\sigma^2)\\). 1.2.3.10 Properties of variance \\(\\mathrm{Var}(aX +b) = a^2 \\mathrm{Var}(X)\\) 1.3 Multivariate random variables 1.3.0.1 Overview Let \\(W = (X, Y) \\in \\mathcal{B}\\) be a multivariate random variable. \\(X\\) and \\(Y\\) can be continuous or discrete. For example \\(X,Y\\) can be multivariate normal. 1.3.0.2 Example For \\(X\\in\\{1,2\\}\\) and \\(Y \\in \\{1,2,3\\}\\) Joint probability table Marginal probabilities Conditional probabilities (intuitively) Draw the 3 different tables. Code probs = data.frame( X=rep(1:3, each=2), Y = rep(c(1,2), 3)) probs$p = paste0(&#39;$p_{&#39;, probs$X, probs$Y, &#39;}$&#39;) tabout = t(table(probs$X, probs$Y)) tabout[,] = probs$p tabout = t(tabout) knitr::kable(tabout, row.names = TRUE, escape = FALSE ) 1 2 1 \\(p_{11}\\) \\(p_{12}\\) 2 \\(p_{21}\\) \\(p_{22}\\) 3 \\(p_{31}\\) \\(p_{32}\\) 1.3.0.3 Example: Umbrella and raining A common example is whether someone brings an umbrella with them It’s not raining It’s raining I don’t bring umbrella 0.05 0.17 I bring umbrella 0.35 0.43 Another example It’s not precipitating It’s raining It’s snowing I don’t bring umbrella 0.03 0.17 0.02 I bring umbrella 0.35 0.40 0.03 1.3.1 Distribution functions The distribution function for multivariate random variables is often written in terms of subsets \\(B \\subset \\mathcal{B}\\) \\[ F_W(B) = \\mathbb{P}(W \\in B). \\] If \\(W\\) has a density then we can also write for values \\(x\\) and \\(y\\) (in the appropriate domain) \\[ \\mathbb{P}(W \\in B) = \\int_{B} f_W(x, y)dx dy, \\] where \\(f_W\\) is the PDF or PMF of \\(W = (X,Y)\\). 1.3.2 Marginal probability The marginal probability, \\(\\mathbb{P}(Y=y)\\) is the probability for \\(Y\\) ignoring whatever is going on with \\(X\\). This can be found by integrating out the \\(X\\) variable. \\[ f_Y(y) = \\int_\\mathcal{X} f_{X,Y}(x, y)dx \\] Joint multinomial above to demonstrate this. 1.3.3 Conditional distribution Conditional distributions can be written in terms of the underlying probability space. For sets \\(A, B \\subset \\mathcal{B}\\) \\[ \\mathbb{P}(A \\mid B ) = \\frac{\\mathbb{P}( A \\cap B)}{\\mathbb{P}(B)} \\] Venn diagram of sets Conceptually, the denominator adjusts the numerator for the fact that we are only considering events that include B occurring. This makes the conditional probabilities sum to 1. For multivariate random variables the definition is similar \\[ \\mathbb{P}(Y=y \\mid X=x) = \\frac{\\mathbb{P}(X=x, Y=y)}{\\mathbb{P}(X=x)} \\] Can think of the Venn diagram in terms of the joint random variable falling into the set. Probability I bring and umbrella given that it’s raining Probability it’s raining given it’s not precipitating (Conditioning on itself) 1.3.4 Examples 1.3.4.1 Insurance dataset children and region The insurance dataset is in the RESI R package, which you can install with the following code Code install.packages(&#39;RESI&#39;) There are some cool multivariate distributions in this data set. Consider the probability of having a certain number of children living in a certain area. Code library(RESI) # contains insurance dataset childByRegion = table(insurance$region, insurance$children) childByRegion = childByRegion/sum(childByRegion) knitr::kable(round(childByRegion, 2)) 0 1 2 3 4 5 northeast 0.11 0.06 0.04 0.03 0.01 0.00 northwest 0.10 0.06 0.05 0.03 0.00 0.00 southeast 0.12 0.07 0.05 0.03 0.00 0.00 southwest 0.10 0.06 0.04 0.03 0.01 0.01 Let \\(X\\) be a random variable denoting number of children Is it categorical/continuous/ordinal? Let \\(Y\\) be a random variable denoting region Is it categorical/continuous/ordinal? joint distribution is a function of both variables \\(\\mathbb{P}(X=x, Y=y) = f(x,y)\\). Questions about regional differences and number of children can be determined from the table What is the probability of having zero kids and living in the east? What is the probability of having zero kids? 1.3.4.2 Insurance data multivariate distribution of age and charges Code library(RESI) name=&#39;charges&#39; hist(insurance[,name], xlab=name, main=name, freq=FALSE) lines(density(insurance[,name], adjust=1.5), lwd = 2) Relationship between age and charges in the insurance data Code plot(insurance[,&#39;age&#39;], insurance$charges, ylab=&#39;Charges&#39;, xlab=&#39;Age&#39;, main=paste(&#39;Age&#39;, &#39;and&#39;, &#39;Charges&#39;) ) The scatter plot can be thought of as a dataset version of a joint density \\(f_{X,Y}(x,y)\\) We can get the conditional histograms for a few different age groups Code par(mar=c(8,2.8,1.8,.2), mgp=c(6,.7,0)) nbin = 15 insCat = with(insurance, data.frame(age=cut(age, breaks = seq(min(age), max(age), length.out=7), include.lowest=T), charges=cut(charges/1000, breaks = seq(min(charges/1000), max(charges/1000), length.out=nbin), include.lowest=T) ) ) condTab = do.call(cbind, by(insCat$charges, insCat$age, function(x){res=table(x); res/sum(res)})) barplot(condTab, beside=TRUE, col=rep(cols[1:ncol(condTab)], each=nbin-1), names.arg = rep(rownames(condTab), ncol(condTab) ), las=2, xlab=&#39;Charges (in thousands)&#39;, main=&#39;Conditional frequencies&#39;) legend(&#39;topright&#39;, fill=cols[1:ncol(condTab)], legend=colnames(condTab), bty=&#39;n&#39;) Code par(mar=c(8,2.8,1.8,.2), mgp=c(6,.7,0)) nbin = 15 jointTab = do.call(cbind, by(insCat$charges, insCat$age, function(x){res=table(x); res})) jointTab = jointTab/sum(jointTab) barplot(jointTab, beside=TRUE, col=rep(cols[1:ncol(jointTab)], each=nbin-1), names.arg = rep(rownames(jointTab), ncol(jointTab) ), las=2, xlab=&#39;Charges (in thousands)&#39;, main=&#39;Joint frequencies&#39;) legend(&#39;topright&#39;, fill=cols[1:ncol(jointTab)], legend=colnames(jointTab), bty=&#39;n&#39;) Code par(mar=c(8,2.8,1.8,.2), mgp=c(6,.7,0)) nbin = 15 test = barplot(t(jointTab), beside=FALSE, col=cols[rep(1:ncol(jointTab), nbin-1)], las=2, xlab=&#39;Charges (in thousands)&#39;, main=&#39;Marginal frequencies&#39;, space=0) legend(&#39;topright&#39;, fill=cols[1:ncol(jointTab)], legend=colnames(jointTab), bty=&#39;n&#39;) 1.3.5 Conditional means and variances Once you have conditional probabilities, you can do all the same stuff with it that you would do with a regular random variable For example, \\(\\mathbb{E}(Y \\mid X) = \\sum_{y} y f_{y\\mid x}(y \\mid x)\\) Conditional expectation is just an expectation with respect to a conditional distribution. \\[ \\begin{align*} \\mathbb{E}(Y \\mid X=x) &amp; = \\int_{\\mathcal{Y}} yf_{Y\\mid X=x}(y\\mid x)dy \\\\ &amp; = \\int_{\\mathcal{Y}} yf_{Y,X}(y, x)/f(x)dy \\\\ &amp; = \\int_{\\mathcal{Y}} yf_{Y,X}(y, X)/f(X)dy \\end{align*} \\] It can still be thought of as random with respect to the conditioning variable. \\(P(X=1)=.1\\) \\(P(X=2) = .7\\) \\(P(X=3)=.2\\) \\(Y\\) \\(P(Y|X=1)\\) \\(P(Y| X=2)\\) \\(P(Y| X=3)\\) 1 1/4 .1 .7 2 1/4 .2 .1 3 1/4 .3 .1 4 1/4 .4 .1 1.3.6 Properties of expectation for two random variables Expectation is linear because it is an integral. For example, for constants \\(a,b\\) and random variables \\(X,Y\\) \\[ \\mathbb{E}(aX + bY) = a \\mathbb{E} X + b\\mathbb{E}Y \\] 1.3.6.1 Example: discrete and continuous joint distribution Diagnosis and hippocampus volume in Alzheimer’s disease. \\(P(DX)\\) is multinomial, \\(P(Hipp\\mid DX)\\) assumed to be normal. Code adni = readRDS(&#39;../datasets/adni/hippocampus.rds&#39;) tab = round(do.call(rbind, by(adni$LEFTHIPPO, adni$DX, function(x) c(mean(x), var(x)) )), 0) pX = table(adni$DX); pX = as.data.frame(round(pX/sum(pX),2)); names(pX) = c(&#39;X&#39;, &#39;P(X)&#39;) colnames(tab) = c(&#39;Mean&#39;, &#39;Variance&#39;) knitr::kable(pX, row.names = FALSE, escape = FALSE ) X P(X) AD 0.26 HC 0.29 MCI 0.45 Code knitr::kable(tab, row.names = TRUE, escape = FALSE ) Mean Variance AD 1549 109794 HC 2122 89241 MCI 1803 131030 1.3.7 Independence Two random variables are independent if they factor \\(\\mathbb{P}(X=x, Y=y) = \\mathbb{P}(X=x)\\mathbb{P}(Y=y)\\) for all possible values of \\(x\\) and \\(y\\). Intuitively, this means that fixing one variable doesn’t affect the distribution of the other. Another way to express it is that \\(\\mathbb{P}(X=x\\mid Y=y) = \\mathbb{P}(X=x)\\). The table is pretty close to independent. Conditional probabilities are \\(\\mathbb{P}(\\text{N kids} | \\text{Region})\\). This means that they are independent if their CDFs or PDFs factor! If the table above were perfectly independent, it might look like this: Code rs = rowSums(childByRegion) cs = colSums(childByRegion) indepTab = outer(rs, cs) knitr::kable(round(indepTab, 3)) 0 1 2 3 4 5 northeast 0.104 0.059 0.043 0.028 0.005 0.003 northwest 0.104 0.059 0.044 0.029 0.005 0.003 southeast 0.117 0.066 0.049 0.032 0.005 0.004 southwest 0.104 0.059 0.044 0.029 0.005 0.003 Code barplot(indepTab, main=&#39;Joint probabilities&#39;, ylab=&#39;Probability&#39;, xlab=&#39;Number of children&#39;, beside=T, col=cols[1:nrow(indepTab)]) legend(&#39;topright&#39;, fill=cols[1:nrow(indepTab)], legend=rownames(indepTab), bty=&#39;n&#39; ) Code condTab = sweep(indepTab, 1, rowSums(childByRegion), FUN = &#39;/&#39;) barplot(condTab, main=&#39;Conditional probabilities&#39;, ylab=&#39;Probability&#39;, xlab=&#39;Number of children&#39;, beside=T, col=cols[1:nrow(condTab)]) legend(&#39;topright&#39;, fill=cols[1:nrow(condTab)], legend=rownames(condTab), bty=&#39;n&#39; ) Are charges and smoking independent? 1.3.8 Covariance and dependence Expand this and derive the other formula \\[ \\text{Cov}(X, Y) = \\mathbb{E}(X-\\mathbb{E}X)(Y-\\mathbb{E}Y) \\] Correlation is \\[ \\begin{align*} \\text{Cor}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\in [-1,1] \\end{align*} \\] Covariance implies dependence, but dependence does not imply covariance (homework question). Independence implies zero covariance, but dependence does not imply nonzero covariance. The covariance can be zero between two random variables, but they can still be dependent. Covariance summarizes the linear dependence between two random variables. 1.3.9 Pearson’s correlation Sampling two measurements on each subject \\(X_i\\) \\(Y_i\\). Correlation is a normalized variance \\[ \\begin{align*} \\rho = \\text{Cor}(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}} \\end{align*} \\] It takes values between -1 and 1. negative values means they two variables have a negative (approximately) linear relationship positive means they have a positive (approximately) linear relationship Correlation is not slope. A slope of zero implies no correlation correlation Can estimate correlation with \\[ \\begin{align*} \\hat\\rho = \\frac{\\widehat{\\text{Cov}}(X,Y)}{\\sqrt{\\widehat{\\text{Var}}(X)\\widehat{\\text{Var}}(Y)}} = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sqrt{\\sum_{i=1}^n(X_i - \\bar X)^2\\sum_{i=1}^n(Y_i - \\bar Y)^2}} \\end{align*} \\] 1.3.9.1 Examples Code plot(insurance[,&#39;age&#39;], insurance$charges, ylab=&#39;Charges&#39;, xlab=&#39;Age&#39;, main=paste(&#39;Age&#39;, &#39;and&#39;, &#39;Charges&#39;) ) Code cor(insurance[,&#39;age&#39;], insurance$charges) ## [1] 0.2990082 1.3.9.2 Examples Dependent but uncorrelated Code par(mfrow=c(2,1)) X = rnorm(100) Y = X^2 plot(X, Y, main=&#39;Uncorrelated dependent no noise&#39;) abline(lm(Y ~ X)) cor(X,Y) ## [1] 0.1224623 Code X = rnorm(100) Y = X^2 + rnorm(100) plot(X, Y, main=&#39;Uncorrelated dependent with noise&#39;) abline(lm(Y ~ X)) Code cor(X,Y) ## [1] -0.1366219 1.3.10 Properties of variances of sums Derive the variance of this \\(\\text{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\text{Cov}(X, Y)\\) 1.3.11 Law of total expectation The law of total expectation is relatively simple, but super handy. \\[ \\mathbb{E}\\{\\mathbb{E}(X \\mid Y)\\} = \\mathbb{E}X \\] If you are trying to take a mean of a complicated random variable, then you can condition on part of it first and then take the expectation of the part you conditioned on. Example: continue from last table Example: Use diagnosis and hippocampal volume above Example from Rice: 1.3.12 Law of total variance The law of total variance states that \\[ \\text{Var}(Y) = \\mathbb{E}\\text{Var}(Y \\mid X) + \\text{Var}\\{ \\mathbb{E}(Y \\mid X)\\}. \\] Example: continue from last table Example: Use diagnosis and hippocampal volume above 1.4 Parameters, estimates, and estimators 1.4.1 Objectives Talk about parameters, estimates, and estimators Talk about how to assess estimators by comparing their variance and bias 1.4.2 Example: healthy sleep Startsleeping.org For this example let’s look at the sleep questionnaire from the HCP What proportion of people get healthy sleep (assuming 7-9 hours). Code hcp = read.csv(&#39;../datasets/hcp/hcp.csv&#39;) hist(hcp$PSQI_AmtSleep, main=&#39;Nightly sleep&#39;, xlab=&#39;Hours&#39;) Code hcp$healthySleep = ifelse(hcp$PSQI_AmtSleep&lt;=9 &amp; hcp$PSQI_AmtSleep&gt;=7, 1, 0) 1.4.3 Statistics is a way to learn about the world from a data set All three statistical philosophies we will discuss view research questions as trying to learn about an unknown parameter. For now, let’s work with the question, “What is the proportion of people in the united states who get a healthy amount of sleep?” What is my parameter of interest (target parameter)? Descriptively – proportion of people who get a healthy amount of sleep Mathematically – call this value \\(p\\) What is my population of interest? Descriptively – The United States population Quantitatively two possible things Distribution defined by the model below in point 4. \\(\\text{Be}(p)\\) Unknown statistical distribution that gives rise to whether someone reports between 7-9 hours of sleep. What is a data point? An answer from an individual in the population to the survey question. Coded as 1/0 if they do/don’t get 7-9 hours of sleep. What model can I use to describe how I get a data point? Most obvious one is \\(X_i \\sim \\text{Be}(p)\\), where \\(p\\) is the parameter in point 1. 1.4.4 A random sample A random sample is a collection of independent random variables that represent potential data points. Let \\(X_i \\sim \\text{Be}(p)\\) for \\(i=1,\\ldots, n\\). Our assumption about the population \\(X_i\\) is drawn from connects the data to the parameter of interest. Given our data, we have a parameter and estimate of the parameter. What are they? 1.4.5 Estimates Note, an estimate is a function of the observed sample and it is nonrandom (Why is it non random?). We often use lowercase letters to make that clear \\[ \\bar x = n^{-1} \\sum_{i=1}^n x_i. \\] 1.4.6 Estimators An estimator is a function of a random sample. The goal (usually) is to estimate a parameter of interest (here, \\(p\\)). Let’s consider the estimator \\[ \\hat p = \\bar X = n^{-1} \\sum_{i=1}^n X_i, \\] which we know is pretty reasonable since \\(\\mathbb{E}\\bar X = p\\). We can study the properties of estimators to learn about how they behave. If we like an estimator we can compute an estimate using our sample. We can use features of our estimator to make probabilistic statements about what might happen if we repeat our study. Dataset-to-dataset variability This perspective is considered to be the Frequentist philosophy of Statistics. 1.4.6.1 Properties of estimators Some common metrics are Bias: let \\(p\\) denote our target parameter, bias is defined as \\[ \\mathbb{E}(\\hat p - p). \\] Variance: this is the same as for other random variables \\[ \\mathbb{E}(\\hat p - \\mathbb{E}\\hat p)^2 \\] Mean Squared Error: this combines variance and bias: \\[ \\mathbb{E}(\\hat p - p)^2 \\] Consistency: this describes what happens to the estimator as \\(n\\to\\infty\\) \\[ \\hat p \\to_P p \\] The arrow is convergence in probability. More on this later. Asymptotically unbiased: is a less strong statement of what happens to the estimator as \\(n\\to\\infty\\) \\[ \\mathbb{E}\\hat p \\to p \\] 1.4.6.2 Bias, variance, and MSE of the proportion estimator The bias of the proportion estimator (used to estimate the proportion of people who get enough sleep) is: \\[ \\mathbb{E}( \\hat p - p) = 0 \\] The variance of the estimator is \\[ \\begin{align*} \\text{Var}(\\hat p) &amp; = n^{-2} \\sum_{i=1}^n\\text{Var}(X_i) \\\\ &amp; = n^{-1} p(1-p) \\end{align*} \\] The MSE is the same as the variance since \\(\\hat p\\) is unbiased. We will come back to this example to construct confidence intervals later. 1.4.7 HCP: example of bias, variance, and MSE In HCP data Code # Show what Bias and Variance mean in NSDUH data using simulations nstudies = 100000 p = 0.6 n = 100 phats = rep(NA, nstudies) for(study in 1:nstudies){ # random sample x = rbinom(n = n, prob = p, size=1) # estimate phat phats[study] = mean(x) } # mean of phat across the simulations mean(phats) ## [1] 0.5998757 Code hist(phats) Code # variance of phat across the simulations var(phats) ## [1] 0.002404844 Code # what the variance should be equal to p * (1-p)/n ## [1] 0.0024 1.4.8 Parameters, Estimators, Estimates To reiterate: Parameter – Target unknown feature of a population (nonrandom) Estimate – Value computed from observed data to approximate the parameter (nonrandom) Estimator – A function of a random sample to approximate the parameter In statistics, probability is used to define and describe the behavior of estimators. Often, the distribution of an estimator is makes it too hard to find the bias, variance, and MSE of the estimator. In this case, we use simulations to estimate the bias, variance, and MSE of an estimator. Important concepts for this session: Simulations can be used to estimate the bias and variance when it is too hard to find mathematically. Sometimes, the parameter of interest is not distributional parameter. 1.4.9 Another example: multiplexed immunofluorescence (mIF) microscopy mIF imaging uses antibody markers to identify protein in tissue samples. The protein Beta-catenin is known to be higher in tumor cells. beta catenin image 1.4.10 mIF imaging in a statistical framework Suppose we want to ask the question, “What is the mean concentration of Beta-catenin in the cells from a given tissue sample?” What is my parameter of interest (target parameter)? Mean cellular concentration of Beta-catenin \\(\\mathbb{E}X_i\\) What is my population of interest? All possible cells from the given tumor (potentially)? All possible cells from similar tumors? What is a data point? A single cell What model can I use to describe how I get a data point? That’s a good question. Here is the histogram of the data Code bc = readRDS(&#39;../datasets/betaCatenin.rds&#39;) hist(bc, main=&#39;Beta Catenin Histogram&#39;, xlab=&#39;Marker count&#39;) + The Beta-catenin concentration across cells could be modeled with a Gamma distribution. 1.4.11 A Gamma model for Beta-catenin concentration Previously, we defined the parameter of interest \\(p\\) based on what we were interested in the sleep problem and it also happened to be the parameter of the Bernoulli distribution. In this example that is not the case. Let \\(X_i\\) denote a randomly drawn cell from the tissue image. Assume \\(X_i \\sim \\text{Gamma}(\\alpha, \\beta)\\). Let’s consider three parameters \\(\\mathbb{E}X_i = \\alpha/\\beta\\) \\(\\alpha\\) (shape parameter) \\(\\beta\\) (rate parameter) Let’s assess estimators for each of these parameters. 1.4.12 MxIF example: Estimators I used method of moments (which you’ll learn in another class) to obtain estimators for these parameters. The estimators are \\[ \\begin{align*} \\hat \\mu &amp; = \\bar X = n^{-1} \\sum_{i=1}^n X_i \\\\ \\tilde \\alpha &amp; = \\frac{\\bar X^2}{\\left(\\overline{X^2} - \\bar X^2\\right)} \\\\ \\tilde \\beta &amp; = \\frac{\\bar X}{\\left(\\overline{X^2} - \\bar X^2\\right)} \\end{align*} \\] \\(\\tilde \\alpha\\) and \\(\\tilde \\beta\\) are complicated functions of random variables involving ratios. It might be hard to find their bias. It will definitely be hard to find their variance. Instead, let’s use simulations to assess the bias, variance, and MSE of these estimators. 1.4.13 Concept of simulation study The bias, variance, and MSE are defined with respect to the distribution of the statistic across repeated samples of the data. If I can repeat the experiment in R multiple times, then I can see multiple random versions of the estimator. A simulation is an experiment to study what happens across experiments. 1.4.14 Simulation study Code set.seed(100) # number simulation nsim = 10000 # sample sizes ns = c(10, 25, 50, 100, 200, 500) # values of alpha to consider alphas = c(0.5, 5) # values of beta to consider betas = c(0.25, 3) # presults = data.frame(p=ps, biasmuHat = rep(NA, np), biassigmaSqHat=rep(NA, np), biassigmaSqHat2=rep(NA, np), # varmuHat=rep(NA, np), varsigmaSqHat=rep(NA, np), varsigmaSqHat2=rep(NA, np)) presults = expand.grid(alpha = alphas, beta=betas, n=ns) colNames = paste(rep(c(&#39;muHat&#39;, &#39;alphaTilde&#39;, &#39;betaTilde&#39;), each=3), rep(c(&#39;Bias&#39;, &#39;Variance&#39;, &#39;MSE&#39;), 3)) presults[, colNames ] = NA alphaind = 1; betaind = 2; nind = 3 # loops through the parameter values for(alphaind in 1:length(alphas) ){ for(betaind in 1:length(betas)){ # data generating distribution parameters (for the gamma distribution) alpha = alphas[alphaind] beta = betas[betaind] for(nind in 1:length(ns)){ # get n for this simulation setting n = ns[nind] cat(which(presults$n==n &amp; presults$alpha==alpha &amp; presults$beta == beta), &#39;\\t&#39;) # loops through the simulations # each simulation is one realization of the world results = data.frame(muHat = rep(NA, nsim), alphaTilde=rep(NA, nsim), betaTilde=rep(NA, nsim) ) for(sim in 1:nsim){ # sample a data set of size n from a random variable whose distribution is determined by alpha and beta x = rgamma(n, shape=alpha, rate=beta) # compute estimators muHat = mean(x) alphaTilde = muHat^2 / var(x) betaTilde = muHat / var(x) #sigmaSqHat = sum((x-mean(x))^2)/(length(x)-1) results[sim, c(&#39;muHat&#39;, &#39;alphaTilde&#39;, &#39;betaTilde&#39;) ] = c(muHat, alphaTilde, betaTilde) } # end of the nsim loop # saving the results for each value of p #presults[pind, c(&#39;biasmuHat&#39;, &#39;biassigmaSqHat&#39;, &#39;biasSigmaSqHat2&#39;)] = colMeans(results) - c(p, p*(1-p), p*(1-p)) #presults[pind, c(&#39;varmuHat&#39;, &#39;varsigmaSqHat&#39;, &#39;varSigmaSqHat2&#39;)] = diag(var(results)) # Bias variance MSE for muHat presults[ which(presults$n==n &amp; presults$alpha==alpha &amp; presults$beta == beta), c(&#39;muHat Bias&#39;, &#39;muHat Variance&#39;, &#39;muHat MSE&#39;)] = c(mean(results$muHat) - alpha/beta, var(results$muHat), (mean(results$muHat) - alpha/beta)^2 + var(results$muHat) ) presults[ which(presults$n==n &amp; presults$alpha==alpha &amp; presults$beta == beta), c(&#39;alphaTilde Bias&#39;, &#39;alphaTilde Variance&#39;, &#39;alphaTilde MSE&#39;)] = c(mean(results$alphaTilde) - alpha, var(results$alphaTilde), (mean(results$alphaTilde) - alpha)^2 + var(results$alphaTilde) ) presults[ which(presults$n==n &amp; presults$alpha==alpha &amp; presults$beta == beta), c(&#39;betaTilde Bias&#39;, &#39;betaTilde Variance&#39;, &#39;betaTilde MSE&#39;)] = c(mean(results$betaTilde) - beta, var(results$betaTilde), (mean(results$betaTilde) - beta)^2 + var(results$betaTilde) ) } # loop through ns } # loop through the betas } # loop through the alphas ## 1 5 9 13 17 21 3 7 11 15 19 23 2 6 10 14 18 22 4 8 12 16 20 24 Code subres = presults[ presults$alpha==0.5 &amp; presults$beta ==0.25,] layout(matrix(1:6, nrow=2, byrow=TRUE)) # Bias plots plot(subres$n, subres[, &#39;muHat Bias&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of muHat&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;alphaTilde Bias&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of alphaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;betaTilde Bias&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of betaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;muHat Variance&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Variance&#39;, main=&#39;Variance of muHat&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;alphaTilde Variance&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Variance&#39;, main=&#39;Variance of alphaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;betaTilde Variance&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Variance&#39;, main=&#39;Variance of betaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) Code subres = presults[ presults$alpha==5 &amp; presults$beta ==3,] layout(matrix(1:6, nrow=2, byrow=TRUE)) # Bias plots plot(subres$n, subres[, &#39;muHat Bias&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of muHat&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;alphaTilde Bias&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of alphaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;betaTilde Bias&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of betaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;muHat Variance&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Variance&#39;, main=&#39;Variance of muHat&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;alphaTilde Variance&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Variance&#39;, main=&#39;Variance of alphaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;betaTilde Variance&#39;], xlab=&#39;Sample size&#39;, ylab=&#39;Variance&#39;, main=&#39;Variance of betaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) Percent bias instead of raw bias Code subres = presults[ presults$alpha==0.5 &amp; presults$beta ==0.25,] layout(matrix(1:6, nrow=2, byrow=TRUE)) # Bias plots plot(subres$n, subres[, &#39;muHat Bias&#39;]/(subres[,&#39;alpha&#39;]/subres[,&#39;beta&#39;]), xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Percent bias of muHat&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;alphaTilde Bias&#39;]/subres[,&#39;alpha&#39;]*100, xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Percent bias of alphaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, subres[, &#39;betaTilde Bias&#39;]/subres[,&#39;beta&#39;]*100, xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Percent bias of betaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, sqrt(subres[, &#39;muHat Variance&#39;])/(subres[,&#39;alpha&#39;]/subres[,&#39;beta&#39;])*100, xlab=&#39;Sample size&#39;, ylab=&#39;Percentage&#39;, main=&#39;Percent SE of muHat&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, sqrt(subres[, &#39;alphaTilde Variance&#39;])/subres[,&#39;alpha&#39;]*100, xlab=&#39;Sample size&#39;, ylab=&#39;Percentage&#39;, main=&#39;percent SE of alphaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) plot(subres$n, sqrt(subres[, &#39;betaTilde Variance&#39;])/subres[,&#39;beta&#39;]*100, xlab=&#39;Sample size&#39;, ylab=&#39;Percentage&#39;, main=&#39;percent SE of betaTilde&#39;, type=&#39;b&#39;) abline(h=0, lty=2, lwd=2) 1.4.15 Assessing estimators in the homework I left more interesting problems for you to do in the homework. There, you will compare two estimators for the same parameter, and pick which one you think is better. 1.4.16 MxIF example: Estimate The estimates in the real data are \\(\\hat\\mu=\\)3985.4, \\(\\tilde\\alpha=\\)8.7, and \\(\\tilde\\beta=\\)0.002. Code bc = readRDS(&#39;../datasets/betaCatenin.rds&#39;) alphaTilde = mean(bc)^2/var(bc) betaTilde = mean(bc)/var(bc) x = seq(from=min(bc), to=max(bc), length.out=1000) y = dgamma(x, shape=alphaTilde, rate=betaTilde) hist(bc, main=&#39;Beta Catenin Histogram&#39;, freq=FALSE, xlab=&#39;Marker count&#39;, ylim=range(y)) points(x, y, type=&#39;l&#39;) "],["law-of-large-numbers-and-central-limit-theorem.html", "2 Law of Large Numbers and Central Limit Theorem 2.1 Law of Large Numbers 2.2 Central Limit Theorems (CLTs)", " 2 Law of Large Numbers and Central Limit Theorem In this section we will: Introduce the Law of Large Numbers (LLN) We will use simulations to demonstrate the LLN Simulations -&gt; Math Math -&gt; Simulations Then we will create a simulation where the LLN doesn’t work 2.0.1 Conceptual overview This is to illustrate the law of large numbers using some example data. The law of large numbers describes what happens to the empirical average as it is taken over increasing sample sizes. Code # read in student names # each student is a researcher students = read.csv(&#39;../../unshared/students2023.csv&#39;) # read in data puf = readRDS(&#39;../datasets/nsduh/puf.rds&#39;) # ever tried cigarettes indicator triedCigs = puf$cigflag # make it a Bernoulli random variable triedCigs = ifelse(triedCigs==&#39;yes&#39;, 1, 0) ns = c(10, 50, 100, 200, 300, 400, 500) for(n in ns){ # Each person in the class is performing a study of smoking studies = lapply(c(students$First.Name, &#39;Kun&#39;, &#39;Simon&#39;), function(student) sample(triedCigs, size=n)) names(studies) = students$First.Name # get the mean for each person&#39;s study studyMeans = sapply(studies, mean) # histogram of the study means hist(studyMeans, xlim=c(0,1), breaks=10, main=paste0(&#39;n=&#39;, n)) abline(v=mean(triedCigs)) } Code #hist(sqrt(n)*(studyMeans-mean(triedCigs)), xlim=c(0,1), breaks=10, main=paste0(&#39;n=&#39;, n)) 2.1 Law of Large Numbers The LLN is a tool we can use to say that the average of a sample is close to its expected value (and get’s closer with larger sample sizes). I wouldn’t consider an estimator that doesn’t satisfy the LLN 2.1.1 Simulations -&gt; Math: illustrating the LLN with simulations 2.1.1.1 Initial simulation without convergence definition First, let’s choose a distribution. Then, let’s initialize empty vectors, x and means. For n in 1:maxn, Draw a sample from our distribution and add it to x, x&lt;-c(x,&lt;new random sample&gt;). Compute the mean of x and save it in another vector means[n]&lt;- mean(x). Plot the vector means. Code # distribution we&#39;re sampling from p = mean(triedCigs) RVfunc = function(n) rbinom(n, size=1, prob=p) # x is the observed sample x = c() # this is the mean of the observed sample means = c() # maximum sample size to get to maxn=5000 # for loop through samples for(n in 1:maxn){ xn = RVfunc(1) x = c(x, xn) means = c(means, mean(x)) } plot(1:maxn, means, type=&#39;l&#39;, xlab=&#39;&#39;, ylab=&#39;&#39;, ylim=c(0,1)) # what is this converging to? abline(h=p, lty=2) What do you notice about this plot? Let’s describe mathematically what we did. \\(X_i \\sim\\), for \\(n=1,2, \\ldots\\) \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) \\(\\bar X_n \\to\\) what? What does this little arrow mean? 2.1.1.2 Convergence of random variables Definition: \\(Y_n\\) converges to \\(Y\\) in probability if for all \\(\\epsilon&gt;0\\), \\(\\lim_{n\\to\\infty}\\mathbb{P}(\\lvert Y_n - Y \\rvert \\le \\epsilon) = 1\\) as \\(n\\to\\infty\\). “Eventually, every world will have \\(Y_n\\) within \\(\\epsilon\\) of \\(Y\\).” “The probability that \\(Y_n\\) is within \\(\\epsilon\\) of \\(Y\\) goes to 1.” Definition: \\(Y_n\\) converges almost surely to \\(Y\\) if \\(\\mathbb{P}(\\lim_{n\\to \\infty}\\lvert Y_n - Y \\rvert =0) = 1\\) as \\(n\\to\\infty\\). “There is not a world where \\(Y_n\\) does not converge to \\(Y\\).” “The probability \\(Y_n\\) does not converge to \\(Y\\) is zero.” 2.1.2 Math -&gt; Simulations: adding convergence definition How do we incorporate one of these convergence definitions into our simulations? Someone in the class give epsilon&gt;0. For each sim in 1:nsim Then, let’s initialize empty vector, x and empty array means=matrix(NA, nrow=nsim, ncol=maxn). For n in 1:maxn, Draw a sample from our distribution and add it to x, x&lt;-c(x,&lt;new random sample&gt;). Compute the mean of x and save it in another vector means[sim,n]&lt;- mean(x). Use means to compute \\(\\mathbb{P}(\\lvert \\bar X_n - \\mathbb{E}X_i \\rvert &lt; \\epsilon)\\). Plot \\(\\mathbb{P}(\\lvert \\bar X_n - \\mathbb{E}X_i \\rvert &lt; \\epsilon)\\) as a function of n. Code epsilon = 0.02 nsim = 500 # distribution we&#39;re sampling from p = mean(triedCigs) RVfunc = function(n) rbinom(n, size=1, prob=p) # maximum sample size to get to maxn=10000 # this is the mean of the observed sample meansout = matrix(NA, nrow=nsim, ncol=maxn) for(sim in 1:nsim){ cat(sim, &#39;\\t&#39;) # for loop through samples # x is the observed sample x = c() means = c() x = RVfunc(maxn) meansout[sim,] = cumsum(x)/1:maxn # for(n in 1:maxn){ # xn = RVfunc(1) # x = c(x, xn) # means = c(means, mean(x)) # } # meansout[sim,] = means } ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 Code P_XbarMinusp = colMeans(abs(meansout - p) &lt; epsilon) plot(1:maxn, P_XbarMinusp, type=&#39;l&#39;, xlab=&#39;&#39;, ylab=&#39;&#39;, ylim=c(0,1)) Code # plot(1:maxn, means, type=&#39;l&#39;, xlab=&#39;&#39;, ylab=&#39;&#39;) # what is this converging to? # abline(h=, lty=2) 2.1.3 Formal definition of LLN 2.1.3.1 IID random variables IID stands for independent and identically distributed. For a sample \\(X_i, \\ldots, X_n\\): Independence means that their probability distributions factor Identical means \\(X_i \\sim F_x\\) (They all have the same distribution function). 2.1.3.2 Law of large numbers (Weak law) Weak LLN is about convergence in probability Theorem (Durrett, pg. 61): Let \\(X_1, X_2, \\ldots\\), be iid random variables with \\(\\mathbb{E}X_i = \\mu\\) and \\(\\mathbb{E}\\lvert X_i \\rvert &lt;\\infty\\). If \\(S_n = X_1 + \\ldots + X_n\\), then as \\(n\\to \\infty\\), \\[ S_n/n \\to_p \\mu. \\] Better weak laws can allow some dependence among the observations and usually just require that \\(\\mathbb{E}X_i^2 &lt; \\infty\\) (a slightly stronger assumption than \\(\\mathbb{E}\\lvert X_i \\rvert &lt;\\infty\\)). Check out google if you’re interested. The strong law is about almost sure convergence and doesn’t require any further assumptions. Intuition about LLN. What is the mean (expected value) of \\(S_n/n\\)? What is the variance of \\(S_n/n\\)? Mean Variance 2.1.3.3 How do we use theorems in research? Demonstrate that assumptions of theorem are satisfied for our case. Draw conclusion about our estimator. What if assumptions are violated? The average may or may not converge There are more precise statements that are necessary and sufficient. 2.1.4 Breaking the LLN: Cauchy distribution What is a Cauchy distribution (ratio of normal random variables; t RV on 1 DoF) The LLN can break if \\(\\mathbb{E}\\lvert X_i \\rvert\\) doesn’t exist (is infinite). What does this mean? It does not have any moments (\\(\\mathbb{E}X_i = \\infty\\)). Let’s use the simulations we wrote above to show that the mean of Cauchy random variables does not converge. Code epsilon = 0.01 nsim = 500 # Changed this to a Cauchy random variable! RVfunc = function(n) rcauchy(n) # maximum sample size to get to maxn=5000 # this is the mean of the observed sample meansout = matrix(NA, nrow=nsim, ncol=maxn) for(sim in 1:nsim){ cat(sim, &#39;\\t&#39;) # for loop through samples # x is the observed sample x = RVfunc(maxn) means = cumsum(x)/1:maxn meansout[sim,] = means } ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 Code P_XbarMinusp = colMeans(abs(meansout)&lt;epsilon) plot(1:maxn, P_XbarMinusp, type=&#39;l&#39;, xlab=&#39;&#39;, ylab=&#39;&#39;, ylim=c(0,1)) Code plot(1:maxn, means, type=&#39;l&#39;, xlab=&#39;n&#39;, ylab=&#39;Xbar&#39;) Note: we’ve already taken advantage of the LLN in the class. Does anyone know how? 2.2 Central Limit Theorems (CLTs) Today we’ll consider another theorem about a different type of convergence for random variables. The goals for today are: Illustrate the CLT using Simulations -&gt; Math Illustrate the CLT using the Math -&gt; Simulation approach with the Bernoulli distribution 2.2.1 Preliminary: Normalizing sums of random variables The CLT is about things that look like this: \\(X_i \\sim F\\) (IID arbitrary distribution) with \\(\\mathbb{E}X_i = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2\\). \\(\\bar X = n^{-1} \\sum_{i=1}^n X_i\\) \\(Z = \\sqrt{n} (\\bar X - \\mu)/\\sigma\\) 2.2.2 Preliminary: The normal distribution Link Things about the normal distribution: Standard normal \\(Z\\sim N(0,1)\\) often denoted with a \\(Z\\). PDF often denoted by \\(\\phi(z)\\). CDF often denoted by \\(\\Phi(z)\\). For \\(Y \\sim N(\\mu, \\sigma^2)\\), \\((Y-\\mu)/\\sigma \\sim N(0, 1)\\) (often called Z-normalization). \\(\\mathbb{P}(\\lvert Z \\rvert\\le 1.96) = \\Phi(1.96) - \\Phi(-1.96) \\approx 0.95\\). \\(\\mathbb{P}( Z \\le 1.64) = \\Phi(1.64) \\approx 0.95\\). 2.2.3 Conceptual overview This is to illustrate the CLT numbers using some example data. Code # read in student names # each student is a researcher students = read.csv(&#39;../../unshared/students2023.csv&#39;) # read in data puf = readRDS(&#39;../datasets/nsduh/puf.rds&#39;) # ever tried cigarettes indicator triedCigs = puf$cigflag # make it a Bernoulli random variable triedCigs = ifelse(triedCigs==&#39;yes&#39;, 1, 0) mu = mean(triedCigs) sigma = sqrt(var(triedCigs)) ns = c(5, 10, 50, 100, 200, 300, 400, 500) students = rep(c(students$First.Name, &#39;Kun&#39;, &#39;Simon&#39;), 100) layout(matrix(1:8, nrow=2, byrow=TRUE)) for(n in ns){ # Each person in the class is performing a study of smoking studies = lapply(students, function(student) sample(triedCigs, size=n)) names(studies) = students # get the mean for each person&#39;s study studyMeans = sapply(studies, mean) stdMeans = sqrt(n)*(studyMeans - mu)/sigma # histogram of the study means hist(stdMeans, xlim = c(-3,3), breaks=10, main=paste0(&#39;n=&#39;, n)) } 2.2.4 Preliminary: Convergence in distribution Definition: A random variable \\(Y_n\\) with distribution function \\(F_n(y)\\) is said to converge in distribution to a limit \\(Y\\) with distribution function \\(F(y)\\) if, for all \\(y\\) that are continuity points of \\(F\\), \\(F_n(y) \\to F(y)\\). I’ll denote it \\(Y_n\\to_D Y\\). Let’s unpack the notation \\(F_n(y) \\to F(y)\\). This means for any given \\(y\\) and \\(\\epsilon&gt;0\\) there exists \\(N\\) such that for all \\(n\\ge N\\), \\[ \\lvert F_n(y) - F(y)\\rvert &lt; \\epsilon. \\] In other words, the distance between the distributions goes to zero as \\(n\\) gets larger. 2.2.5 The Central Limit Theorem The Central Limit Theorem (Durrett, pg. 124): Let \\(X_1, X_2, \\ldots\\) be iid with \\(\\mathbb{E}X_i = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2 \\in (0, \\infty)\\). If \\(\\bar X_n = n^{-1} \\sum_{i=1}^n X_i\\), then \\[ n^{1/2}(\\bar X_n - \\mu)/\\sigma \\to_D X, \\] where \\(X \\sim N(0,1)\\). Comments: We need the variance to be finite (stronger assumptions than LLN) 2.2.6 The Lindeberg-Feller Theorem The Lindeberg-Feller Theorem (Wikipedia): Let \\(X_i\\) be independent random variables with \\(\\mathbb{E}X_i = \\mu _i\\) and variances \\(\\text{Var}(X_i) = \\sigma^2_i \\in (0, \\infty)\\). Let \\(\\sigma^2_n = \\sum_{i=1}^n \\sigma^2_i\\). If this sequence of random variables satisfies Lindeberg’s condition \\[ \\lim_{n\\to\\infty}\\sigma^{-2}_n \\sum_{i=1}^n \\mathbb{E}\\left\\{(X_i-\\mu_i)^2 I(\\vert X_i - \\mu_i\\rvert&gt;\\epsilon \\sigma_n) \\right\\} = 0, \\] for all \\(\\epsilon&gt;0\\). Then \\[ Z_n = \\sum_{i=1}^n (X_i-\\mu_i)/\\sigma_n \\to_D Z, \\] where \\(Z\\sim N(0,1)\\). Comments: Relaxes iid assumption to just independence! CLTs are well studied, there are other ones for some what dependent variables. Will review CLT statement Will look at what it means for Bernoulli example and using simulations 2.2.7 Example of CLT with Bernoulli distribution The Bernoulli is appealing because we can assess the CLT mathematically. Sum of Bernoulli RV is Binomial. Using the CLT, for samples \\(X_1, \\ldots, X_n\\), from this distribution the standardized mean is \\[ \\sqrt{n}\\frac{(\\bar X_n -p)}{p(1-p)} \\sim N(0,1) \\text{ (approximately)} \\] \\(\\mathbb{P}(\\sqrt{n} (\\bar X - p)/\\sqrt{p(1-p)} \\le z) = \\mathbb{P}\\left(n\\bar X \\le \\left(np(1-p)\\right)^{1/2}\\times z + np \\right) \\to \\mathbb{P}(Z\\le z)\\), where \\(Z\\sim N(0,1)\\) So how do we evaluate this: Pick a value for \\(p\\). Choose a vector for \\(n\\) and \\(z\\). Compare the CDFs by the sample size. Note: We don’t even need to run simulations because we can do it using the CDFs in R. Code ns = 1:5000 p = 0.01 z = seq(-2, 2, length.out=1000) probDiffs = rep(NA, length(ns)) for(n in ns){ probDiffs[n] = max(abs( pbinom(sqrt(n*p*(1-p))*z + n*p, size = n, prob=p) - pnorm(z) ) ) } plot(ns, probDiffs, xlab=&#39;Sample size&#39;, ylab=&#39;Distribution error&#39;, main=&#39;CLT for Bernoulli&#39;, type=&#39;l&#39;) abline(h=0, lty=2) Code ns = 1:5000 p = 0.01 z = 1.64 probDiffs = rep(NA, length(ns)) for(n in ns){ probDiffs[n] = abs( pbinom(sqrt(n*p*(1-p))*z + n*p, size = n, prob=p) - pnorm(z) ) } plot(ns, probDiffs, xlab=&#39;Sample size&#39;, ylab=&#39;Distribution error&#39;, main=&#39;CLT for Bernoulli&#39;, type=&#39;l&#39;) abline(h=0, lty=2) 2.2.8 QQ-plot! 2.2.8.1 Binomial proportion: Another look using simulations Code ns = seq(5, 100, by=10) layout(matrix(1:10, ncol=5, byrow=TRUE)) p = 0.75 for(n in ns){ means = replicate(10000, # compute mean 1000 times sqrt(n) * (mean(rbinom(n, 1, p))-p) / sqrt(p*(1-p)) ) hist(means,main=paste(&#39;n =&#39;, n), probability = TRUE, ylab=&#39;&#39;, xlab=&#39;&#39; ) # draw standard normal density x = qnorm(ppoints(1000)) points(x, dnorm(x), type=&#39;l&#39;) } Code layout(matrix(1:10, ncol=5, byrow=TRUE)) for(n in ns){ means = replicate(10000, # compute mean 1000 times sqrt(n)*(mean(rbinom(n, 1, p))-p)/sqrt(p*(1-p)) ) qqnorm(scale(means), main=paste(&#39;n =&#39;, n), ylab=&#39;Means quantiles&#39;, xlab=&#39;Normal quantiles&#39; ) abline(a=0, b=1) } 2.2.9 Comment about dependence in the CLT For a bunch of RVs with zero mean and nonzero covariance \\[ \\begin{aligned} \\text{Var}(n^{-1/2}\\bar X_n) &amp; = n^{-1}\\text{Cov}(S_n, S_n) \\\\ &amp; = \\sum_{i,j}^n \\text{Cov}(X_i, X_j) \\\\ &amp; = n^{-1}\\sum_{i=1}^n \\text{Var}(X_i) + n^{-1}\\sum_{i\\ne j} \\text{Cov}(X_j, X_k)\\\\ &amp; = \\text{Var}(X_i) + (n-1)\\text{Cov}(X_j, X_k)\\\\ &amp;\\text{(if Var and Cov are the same for all RVs)} \\end{aligned} \\] Comments: The first term converges under CLT assumptions. The second term has \\(n(n-1)\\) terms. We need the covariance terms to get “small enough” for the CLT to work. For example, by things that are “far away” having small covariance. 2.2.10 Conclusions of CLT The CLT allows us to make approximate probability statements for things that can be expressed as sums. This is handy when we don’t know the distribution of things. Some form of independence is necessary. 2.2.11 Take home points of LLN and CLT LLN is about the convergence of the mean estimator to a constant. CLT is about the convergence of the mean estimator times \\(\\sqrt{n}\\) to a normal distribution. CLT is used a lot in statistics to make approximate probability statements. 2.2.12 Example: HCP data set A brain. Human Connectome Project data Study designed to understand how regions of the brain are interconnected Code hcp = read.csv(&#39;../datasets/hcp/hcp.csv&#39;) corr = cor(hcp$FS_Total_GM_Vol, hcp$FS_TotCort_GM_Vol, use = &#39;pairwise.complete.obs&#39;) plot(hcp$FS_Total_GM_Vol, hcp$FS_TotCort_GM_Vol, xlab=&#39;Gray Matter Vol&#39;, ylab=&#39;Cortical GM Vol&#39;, main=&#39;Cortical GM Volume vs total GM vol&#39;) legend(&#39;topleft&#39;, legend=paste(&#39;Cor =&#39;, round(corr, 2) )) abline(lm(FS_TotCort_GM_Vol ~ FS_Total_GM_Vol, data=hcp)) Code corr = cor(hcp$FS_SubCort_GM_Vol, hcp$FS_TotCort_GM_Vol, use = &#39;pairwise.complete.obs&#39;) plot(hcp$FS_SubCort_GM_Vol, hcp$FS_TotCort_GM_Vol, xlab=&#39;Subcortical Vol&#39;, ylab=&#39;Cortical GM Vol&#39;, main=&#39;Cortical GM Volume vs Subcortical GM vol&#39;) legend(&#39;topleft&#39;, legend=paste(&#39;Cor =&#39;, round(corr, 2) )) Code #abline(lm(FS_TotCort_GM_Vol ~ FS_Total_GM_Vol, data=hcp)) Code hist(hcp$PSQI_Score) Code corr= cor(hcp$PSQI_Score, hcp$FS_TotCort_GM_Vol, use = &#39;pairwise.complete.obs&#39;) # compute correlation manually PSQI_Score = hcp$PSQI_Score[!is.na(hcp$PSQI_Score) &amp; !is.na(hcp$FS_TotCort_GM_Vol)] TotCorGMVol = hcp$FS_TotCort_GM_Vol[!is.na(hcp$FS_TotCort_GM_Vol) &amp; !is.na(hcp$PSQI_Score)] corr ## [1] -0.09299307 Code cov(PSQI_Score, TotCorGMVol)/sqrt(var(PSQI_Score)*var(TotCorGMVol)) ## [1] -0.09299307 Code sum( (PSQI_Score - mean(PSQI_Score)) *(TotCorGMVol - mean(TotCorGMVol)) ) /sqrt(sum( (PSQI_Score - mean(PSQI_Score))^2) * sum( (TotCorGMVol - mean(TotCorGMVol))^2)) ## [1] -0.09299307 Code plot(hcp$PSQI_Score, hcp$FS_TotCort_GM_Vol, xlab=&#39;PSQI scpre&#39;, ylab=&#39;Cortical GM Vol&#39;, main=&#39;Cortical GM Volume vs PSQI score&#39;) legend(&#39;topleft&#39;, legend=paste(&#39;Cor =&#39;, round(corr, 2) )) abline(lm(FS_TotCort_GM_Vol ~ PSQI_Score, data=hcp)) 2.2.13 Simulated data Code set.seed(100) # number of simulations per n and rho nsim = 2 # sample sizes ns = c(100) # correlation rhos = c(0.1) resultsTab = expand.grid(n=ns, rho=rhos) resultsTab$rhoBias = NA # each column is a draw of x and y for(rhoind in 1:length(rhos)){ rho = rhos[rhoind] Sigma = matrix(c(1, rho, rho, 1), nrow=2) sqrtSigma = svd(Sigma) sqrtSigma = sqrtSigma$u %*% diag(sqrt(sqrtSigma$d)) for(nind in 1:length(ns)){ n = ns[nind] # NEED TO DO SOMETHING HERE # need a vector to store the correlation values in simresults = rep(NA, nsim) for(sim in 1:nsim){ xy = tcrossprod(matrix(rnorm(n*2), ncol=2), sqrtSigma) # YOU NEED TO EDIT THE CODE HERE simresults[sim] = cor(xy[,1], xy[,2]) } # AND SOMETHING ELSE HERE (compute bias and assign in resultsTab) } } "],["confidence-intervals.html", "3 Confidence intervals 3.1 Recap on bias assessment 3.2 Confidence intervals for a mean 3.3 Computing confidence intervals 3.4 T- confidence interval 3.5 Evaluating confidence intervals 3.6 Evaluating confidence intervals under assumption violations 3.7 Assumption violations with T-interval 3.8 Confidence intervals for binomial proportions 3.9 Extra stuff", " 3 Confidence intervals 3.1 Recap on bias assessment Let’s do some more practice on computing bias using simulations and analytically. Consider the NSDUH data where the probability of the event is small. E.g. few people in the population have used PCP Code set.seed(555) puf = readRDS(&#39;../datasets/nsduh/puf.rds&#39;) pcptab = table( puf$pcpflag) pcptab/sum(pcptab) ## ## no yes ## 0.98537708 0.01462292 Code n = 50 # all &quot;no&quot;s pcpsamp = sample(puf$pcpflag, size = n) pcpsamp ## [1] no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no no ## [50] no ## Levels: no yes In cases like this, sometimes people recommend using a modified estimator of \\(p\\), \\[ \\tilde p = \\frac{\\sum_{i=1}^n X_i + 1/2}{n + 1}, \\] which adds 1/2 observation to the “yes” category and 1/2 observation to the “no” category. What is expected value of \\(\\tilde p\\)? Plot the bias of \\(\\tilde p\\). Code n = 1:200 ps = c(0.01, 0.1, 0.25, 0.45) for(p in ps){ ptildeBias = (n * p + 0.5)/(n+1) - p # remove y-axis limits to rescale # divide by p for percent bias plot(n, ptildeBias/p, xlab=&#39;n&#39;, ylab=&#39;Bias(ptilde)&#39;, type=&#39;l&#39;, main=paste0(&#39;p=&#39;,p) ) abline(h=0) } We can also run simulations to assess the bias and the curves should agree with the results that we found in the other plots. Code nsim = 10000 n = 1:200 ns = c(10, 25, 50, 100, 200) ps = c(0.01, 0.1, 0.25, 0.45) for(p in ps){ # sample data for each sample size samps = replicate(nsim, {(rbinom(length(ns), size = ns, p) + 1/2)/(ns+1)} ) # take mean across simulations simBias = rowMeans(samps) - p ptildeBias = (n * p + 0.5)/(n+1) - p # divide by p for percent bias plot(n, ptildeBias/p, xlab=&#39;n&#39;, ylab=&#39;Bias(ptilde)&#39;, type=&#39;l&#39;, main=paste0(&#39;p=&#39;,p) ) points(ns, simBias/p, type=&#39;b&#39;) abline(h=0) } 3.2 Confidence intervals for a mean Today we will: Introduce a confidence interval for normally distributed data We will use spinal cord imaging to study these confidence intervals We will run a simulation to understand what the confidence interval means about dataset-to-dataset variability 3.2.1 Multiple sclerosis in the spinal cord Seth Smith’s research team is developing advanced spinal cord magnetic resonance imaging sequences to study multiple sclerosis (MS). These new sequences are making it possible to study the spinal cord in more detail through development and aging Today our research question is What is the cross-sectional area of the C3 segment of the spinal cord? 3.2.1.1 Spinal cord anatomy The cord section we are looking at is cervical section C3 (near the top). Damage here affects a large portion of the body. 3.2.1.2 Seth’s interest in the spinal cord MS is a progressive disease associated with demyelination of neurons (reduces signaling efficiency) that has significant detriment on many aspects of daily life, with motor symptoms being the most common. MRI has almost exclusively focused on the brain in MS and associated MRI-derived brain lesion measures with severity of MS symptoms Seth pointed out to the MS community that there are lesions in the spinal cord that scientists haven’t paid as much attention to. 3.2.1.3 Characterization of the C3 cross-sectional area What random variable models might be appropriate for these data? Code load(&#39;../datasets/smith_cervical_sc/tpmf.rdata&#39;) cex=1.5 par(mgp=c(1.7,.7,0), lwd=1.5, lend=2, cex.lab=0.8*cex, cex.axis=0.8*cex, cex.main=1*cex, mfrow=c(1,1), mar=c(2.8,2.8,1.8,.2), bty=&#39;l&#39;, oma=c(0,0,2,0)) # histogram of C3 CSA hist(mf$`C3 CSA`, main=&#39;C3 CSA&#39;, xlab=&#39;C3 CSA&#39;, freq=F) lines(seq(min(mf$`C3 CSA`), max(mf$`C3 CSA`), length.out=200), dnorm(seq(min(mf$`C3 CSA`), max(mf$`C3 CSA`), length.out=200), mean = mean(mf$`C3 CSA`), sd = sd(mf$`C3 CSA`))) What is a good model for these data? 3.2.1.4 CIs conceptually The concept of a CI highlights the distinction between a parameter, estimate, and estimator. What is the parameter? What is an estimate of the parameter? What is an estimator of the parameter, notationally? The concept of an estimator allows us to make probabilistic statements about the procedures we execute in our dataset. 3.2.2 Confidence interval definition Confidence intervals are an interval obtained from a sample that contains the true value of the parameter with a given probability. \\[ P\\{L(X_1, \\ldots, X_n) \\le p &lt; U(X_1, \\ldots, X_n) \\} = 1-\\alpha, \\] for a given value \\(\\alpha \\in (0,1)\\). Note what things are random here (the end points). The parameter is fixed. 3.2.3 Known variance confidence interval for the mean Let’s assume our spinal cord data \\(X_1, \\ldots X_n \\sim N(\\mu, \\sigma^2)\\) are IID and normally distributed. The empirical mean is an estimator for this value \\[ \\hat\\mu = n^{-1}\\sum_{i=1} X_i. \\] Let’s (unrealistically) assume that \\(\\sigma^2\\) is known. 3.2.3.1 The interval derivation The sum of IID normals is normal \\(N(\\mu, \\sigma^2/n)\\). This is exact (not an approximation). What is the distribution of \\(\\sqrt{n}(\\hat\\mu - \\mu)/\\sigma\\)? We can construct an \\(\\alpha\\) probability confidence interval for \\(\\mu\\) using this fact. \\[ \\begin{align*} \\mathbb{P}(z_{\\alpha/2} &lt; Z &lt; z_{1-\\alpha/2} ) &amp; = \\Phi(z_{1-\\alpha/2}) - \\Phi(z_{\\alpha/2} ) \\\\ &amp; = 1-\\alpha/2 - \\alpha/2\\\\ &amp;= 1-\\alpha \\end{align*} \\] So we can create the interval \\[ \\begin{align*} \\mathbb{P}(z_{\\alpha/2} &lt; Z &lt; z_{1-\\alpha/2} ) &amp; = \\mathbb{P}(z_{\\alpha/2} &lt; \\sqrt{n}(\\hat\\mu - \\mu)/\\sigma &lt; z_{1-\\alpha/2} ) \\end{align*} \\] 3.3 Computing confidence intervals Recap of constructing normal confidence interval Computing the confidence interval in the spinal cord data 3.3.1 Constructing the confidence interval in the Spinal cord data set Code x = mf$`C3 CSA` alpha = 0.01 # get n n = length(x) # get the mean muHat = mean(x) # get the sd stdev = sqrt(var(x)) # sd(x) # get z_alpha/2 and z_(1-alpha/2) qnorm(1-alpha/2) ## [1] 2.575829 Code zalpha = qnorm(1- alpha/2) # construct the interval interval = c(muHat - zalpha * stdev /sqrt(n), muHat + zalpha * stdev /sqrt(n) ) hist(mf$`C3 CSA`, main=&#39;C3 CSA&#39;, xlab=&#39;C3 CSA&#39;, freq=F) lines(seq(min(mf$`C3 CSA`), max(mf$`C3 CSA`), length.out=200), dnorm(seq(min(mf$`C3 CSA`), max(mf$`C3 CSA`), length.out=200), mean = mean(mf$`C3 CSA`), sd = sd(mf$`C3 CSA`))) abline(v=interval, lty=2, lwd=2) Code # What is the interpretation of this interval The mean in my data set, \\(\\bar X_n\\), lies within this interval with probability 1-alpha. This interval contains the true mean parameter with probability 1-alpha. The procedure we used to create this interval captures the mean parameter 1-alpha% of the time. Across a large number of samples, approximately (1-alpha)% of the intervals will contain the true value of the parameter. 3.3.2 Dataset-to-dataset variability Code mu = 1 sigma = 0.8 n = 500 nsim = 100 alpha = 0.05 CIs = data.frame(mean=rep(NA, nsim), lower=rep(NA, nsim), upper=rep(NA, nsim)) for(sim in 1:nsim){ # draw a random sample X = rnorm(n, mean=mu, sd=sigma) stdev = sd(X) # construct the confidence interval CIs[sim, ] = c(mean(X), mean(X) + c(-1,1)*qnorm(1-alpha/2)*stdev/sqrt(n)) } CIs$gotcha = ifelse(CIs$lower&lt;=mu &amp; CIs$upper&gt;=mu, 1, 2) # range(c(CIs$lower, CIs$upper)) plot(1:nsim, CIs$mean, pch=19, ylim = c(0,2), main=paste(nsim, &#39;experiments&#39;), col=CIs$gotcha ) segments(x0=1:nsim, y0=CIs$lower, y1=CIs$upper, col=CIs$gotcha) abline(h=mu, lty=2 ) 3.3.3 Estimating the variance Almost always, the variance of the data is unknown. We need an estimate of \\(\\sigma^2\\) from the data. Two are common \\[ \\begin{align*} \\tilde \\sigma^2 &amp; = n^{-1} \\sum_{i=1}^n (X_i - \\bar X)^2 \\text{ (maximum likelihood estimator)}\\\\ \\hat \\sigma^2 &amp; = (n-1)^{-1} \\sum_{i=1}^n (X_i - \\bar X)^2 \\text{ (unbiased estimator)}\\\\ \\end{align*} \\] We’ll focus on the second estimator. 3.3.4 Unknown variance approximate interval What is the distribution of \\(\\sqrt{n}(\\hat\\mu - \\mu)/\\hat\\sigma\\)? If we don’t know the distribution, then there is theoretical justification (using Slutsky’s theorem) to say \\[ \\sqrt{n}(\\hat\\mu - \\mu)/\\hat\\sigma \\sim N(0,1) \\text{ (approximately)} \\] Approximately means asymptotically, which means that if we can control how big our sample is, then we can make this approximation as precise as we want by choosing a big enough sample size. This suggests a confidence interval \\[ \\hat \\mu \\pm z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}. \\] 3.4 T- confidence interval Today we will: Find an exact interval for the mean when the data are normally distributed Compare the intervals we’ve derived, so far Discuss our assumptions and learn how to assess the assumptions 3.4.1 Unknown variance T-interval We need some preliminaries: If \\(Z_i \\sim N(0, 1)\\) are iid, then (by definition) \\(X = \\sum_{i=1}^{(n-1)}Z_i^2\\) is Chi-squared distributed on \\((n-1)\\) degrees of freedom. Usually denoted \\(X\\sim \\chi^2(n-1)\\). If \\(Z \\sim N(0, 1)\\) and \\(X\\sim \\chi^2(n-1)\\) are independent, then \\[ T = \\frac{Z}{\\sqrt{X/(n-1)}} \\sim t(n-1). \\] where \\(t(n-1)\\) denotes the t-distribution on \\(n-1\\) degrees of freedom. 3.4.1.1 How does this help us and what does it have to do with beer? We already know that, \\[ \\sqrt{n}(\\bar X - \\mu)/\\sigma \\sim N(0,1). \\] It turns out that \\[ (n-1)\\hat \\sigma^2/\\sigma^2 \\sim \\chi^2(n-1), \\] and (surprisingly!) \\(\\hat \\sigma^2\\) and \\(\\bar X\\) are independent! (Which implies that the two random variables above are also independent) Then the test statistic we’ve been working with is \\[ \\sqrt{n}(\\hat\\mu - \\mu)/\\hat\\sigma = \\sqrt{n}(\\hat\\mu - \\mu)/\\sigma \\times \\sqrt{\\frac{\\sigma^2}{\\hat\\sigma^2} } =_D Z \\times \\sqrt{\\frac{n-1}{X}} =_D T, \\] \\(Z \\sim N(0, 1)\\) and \\(X\\sim \\chi^2(n-1)\\), and \\(T\\sim t(n-1)\\) This is the one-sample t-test (or confidence interval). 3.4.1.2 But what does it have to do with beer!?!? William Gossett published the t-test when he was working for Guinness and was required to publish under a pseudoname. Wikipedia has more. Guinness 3.4.1.3 Possible intervals (so far) Here are our intervals then \\[ \\begin{align*} \\hat \\mu \\pm z_{\\alpha/2} \\times \\sigma/\\sqrt{n} \\\\ \\hat \\mu \\pm z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n} \\\\ \\hat \\mu \\pm t_{n-1,\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}. \\end{align*} \\] Code x = mf$`C3 CSA` alpha = 0.01 # get n n = length(x) # get the mean muHat = mean(x) # get the sd stdev = sqrt(var(x)) # sd(x) # get z_alpha/2 and z_(1-alpha/2) qnorm(alpha/2) ## [1] -2.575829 Code zalpha = qnorm(1- alpha/2) talpha = qt(1-alpha/2, n-1) qt(alpha/2, n-1) ## [1] -2.631565 Code # construct the interval Zinterval = c(muHat - zalpha * stdev /sqrt(n), muHat + zalpha * stdev /sqrt(n) ) Tinterval = c(muHat - talpha * stdev /sqrt(n), muHat + talpha * stdev /sqrt(n) ) hist(mf$`C3 CSA`, main=&#39;C3 CSA&#39;, xlab=&#39;C3 CSA&#39;, freq=F, xlim=c(65,85)) lines(seq(min(mf$`C3 CSA`), max(mf$`C3 CSA`), length.out=200), dnorm(seq(min(mf$`C3 CSA`), max(mf$`C3 CSA`), length.out=200), mean = mean(mf$`C3 CSA`), sd = sd(mf$`C3 CSA`))) abline(v=Zinterval, lty=2) abline(v=Tinterval, lty=2, col=&#39;red&#39;) Code # What is the interpretation of this interval 3.5 Evaluating confidence intervals Review assumptions of our three intervals so far. Weaken the assumptions a little bit more. Assess confidence intervals using simulations. 3.5.1 Checking assumptions What assumptions have we made for what reason? We can relax two of these at the expense of having approximate intervals. 3.5.2 Checking assumptions: Q-Q plot A Q-Q plot compares the distribution of the observed data (sample quantiles) compared to the theoretical distribution (here a standard normal) Rank the data and normalize \\(y_i = (x_{(i)}/ - \\bar x)/{\\tilde \\sigma}\\) Plot is x=qnorm(i/n), y=y_i. If data are normal the points will be on the line Check out more details on Wikipedia These data are surprisingly close to a normal distribution! Code cex=1.5 par(mgp=c(1.7,.7,0), lwd=1.5, lend=2, cex.lab=0.8*cex, cex.axis=0.8*cex, cex.main=1*cex, mfrow=c(1,1), mar=c(2.8,2.8,1.8,.2), bty=&#39;l&#39;, oma=c(0,0,2,0)) qqnorm(scale(mf$`C3 CSA`), main=&#39;Normal Q-Q plot&#39;) abline(a=0, b=1, col=&#39;blue&#39;) 3.5.3 Review of what we just did so far We identified a parameter of interest (population mean of C3 CSA). We made some assumptions about the data. We identified three Frequentist confidence intervals. 3.5.4 Aside: What is the interpretation of these intervals? \\[ \\mathbb{P}(\\hat \\mu - z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n} &lt; \\mu &lt; \\hat \\mu + z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}) \\approx 1-\\alpha \\] …this interpretation is all of Frequentist statistics. 3.5.4.1 Relaxing assumptions (normality and identicality) There’s one theorem we mentioned in class last week that can relax both of these assumptions This suggests that we can basically reduce our assumptions to independence and finite variance (and the extra condition in the theorem). Then we can use this interval a lot of the time: \\[ \\mathbb{P}(\\hat \\mu - z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n} &lt; \\mu &lt; \\hat \\mu + z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}) \\approx 1-\\alpha \\] This depends on 2 approximations: Large sample (asymptotic) normality of the mean “plugging-in” \\(\\hat\\sigma\\) for the unknown parameter \\(\\sigma\\). In practice, it seems that it’s always safer to use the T- distribution over the normal distribution for confidence intervals (more on this later) 3.6 Evaluating confidence intervals under assumption violations Today we will: Estimate a confidence interval in the C3 cross sectional data Run simulations to assess performance of confidence intervals You can use this code to run your simulations for homework 3 Check how sensitive our confidence intervals are to assumptions Define some confidence intervals for the binomial proportion 3.6.1 Confidence intervals for the mean in the spinal cord data set Code hist(mf$`C3 CSA`) Code t.test(mf$`C3 CSA`) ## ## One Sample t-test ## ## data: mf$`C3 CSA` ## t = 81.738, df = 90, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 74.56922 78.28441 ## sample estimates: ## mean of x ## 76.42682 Code m = mean(mf$`C3 CSA`) n = nrow(mf) df = n-1 stdev = sd(mf$`C3 CSA`) m ## [1] 76.42682 Code df ## [1] 90 Code # alpha = 0.05 m + c(-1, 1)*qt(0.975, df = df)*stdev/sqrt(n) ## [1] 74.56922 78.28441 3.6.2 Two types of confidence intervals \\[ \\begin{align*} \\hat \\mu \\pm z_{\\alpha/2} \\times \\sigma/\\sqrt{n} \\text{ (don&#39;t know sigma)} \\\\ \\hat \\mu \\pm z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n} \\\\ \\hat \\mu \\pm t_{n-1,\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}. \\end{align*} \\] This interval \\(\\hat \\mu \\pm t_{n-1,\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}\\) is approximate/exact. What are the assumptions? This interval \\(\\hat \\mu \\pm z_{\\alpha/2} \\times \\hat\\sigma/\\sqrt{n}\\) is approximate/exact. Assuming you are using the central limit theorem, the assumptions are 3.6.3 Simulation setup We run simulations to evaluate the confidence intervals in finite samples. What are the metrics we care about, here? Coverage – an \\(\\alpha\\) level Frequentist interval should contain the true parameter at least \\(1-\\alpha\\) percent of the time. Width – a smaller interval (that has the right coverage) is better. How do we setup the simulations? We have a few variables we can set: \\(n\\), \\(\\mu\\), \\(\\sigma^2\\), \\(\\alpha\\), number of simulations. Need a way to sample the data \\(X_i\\) given \\(n\\), \\(\\mu\\), and \\(\\sigma^2\\). Need a function to construct the CIs given the sample. Need a function to assess the coverage and width of the confidence intervals. Need a way to assess violations of normality. Need to do simulations for some different values of some of the parameters. Need to create some nice looking plots for the results. Code mu = 1 sigma = 1 ns = (1:10) * 10 nsim = 5000 alpha = 0.05 # to simulate the null in the ECT data ect = read.csv(&#39;../datasets/ect/ect.csv&#39;) ECT = ect$cgi.i mu = mean(ECT) sigma = sd(ECT) simResults = data.frame(n = ns) simResults[ , c(&#39;Zexact_coverage&#39;, &#39;Zapprox_coverage&#39;, &#39;T_coverage&#39;, &#39;Zexact_width&#39;, &#39;Zapprox_width&#39;, &#39;T_width&#39;)] = NA for(n in ns){ CIs = data.frame(&#39;mean&#39; = rep(NA, nsim)) CIs[, paste(rep(c(&#39;Zexact&#39;, &#39;Zapprox&#39;, &#39;T&#39;), each=2), rep(c(&#39;lower&#39;, &#39;upper&#39;), 3), sep=&quot;_&quot; )] = NA for(sim in 1:nsim){ # draw a random sample, normal or skewed X = sample(ECT, n, replace=TRUE) #mu + (rgamma(n, shape = 1.2, rate = 5) - 1.2/5)/sqrt(1.2)*5 # rnorm(n, sd=sigma) # # sample(n, fakeECT, replace=TRUE) stdev = sd(X) # construct the Zexact confidence interval CIs$mean[sim] = mean(X) CIs[sim, c(&#39;Zexact_lower&#39;, &#39;Zexact_upper&#39;) ] = c(mean(X) + c(-1,1)*qnorm(1-alpha/2)*sigma/sqrt(n)) # construct two more CIs # Z approximate interval CIs[sim, c(&#39;Zapprox_lower&#39;, &#39;Zapprox_upper&#39;) ] = c(mean(X) + c(-1,1)*qnorm(1-alpha/2)*stdev/sqrt(n)) # T-interval CIs[sim, c(&#39;T_lower&#39;, &#39;T_upper&#39;) ] = c(mean(X) + c(-1,1)*qt(1-alpha/2, n-1)*stdev/sqrt(n)) } CIs$Zexact_gotcha = ifelse(CIs$Zexact_lower&lt;=mu &amp; CIs$Zexact_upper&gt;=mu, 1, 0) CIs$Zapprox_gotcha = ifelse(CIs$Zapprox_lower&lt;=mu &amp; CIs$Zapprox_upper&gt;=mu, 1, 0) CIs$T_gotcha = ifelse(CIs$T_lower&lt;=mu &amp; CIs$T_upper&gt;=mu, 1, 0) # This computing the width of the confidence interval CIs[paste(c(&#39;Zexact&#39;, &#39;Zapprox&#39;, &#39;T&#39;), &#39;width&#39;, sep=&quot;_&quot;)] = CIs[, grep(&#39;_upper&#39;, names(CIs))] - CIs[, grep(&#39;_lower&#39;, names(CIs))] simResults[which(simResults$n==n), grep(&#39;coverage&#39;, names(simResults), value = TRUE)] = colMeans(CIs[ , grep(&#39;_gotcha&#39;, names(CIs), value=TRUE) ] ) simResults[which(simResults$n==n), grep(&#39;width&#39;, names(simResults), value = TRUE)] = colMeans(CIs[ , grep(&#39;_width&#39;, names(CIs), value=TRUE) ] ) } plot(simResults$n, simResults$Zexact_coverage, ylim=c(0.5, 1), type=&#39;b&#39;, ylab=&#39;Coverage (proportion)&#39;, xlab=&#39;n&#39;, main=&#39;CI coverage&#39;) points(simResults$n, simResults$Zapprox_coverage, type=&#39;b&#39;, col=&#39;red&#39;) points(simResults$n, simResults$T_coverage, type=&#39;b&#39;, col=&#39;blue&#39;) abline(h=1-alpha, lty=2) legend(&#39;bottomright&#39;, legend=c(&#39;Zexact&#39;, &#39;Zapprox&#39;, &#39;T&#39;), col=c(&#39;black&#39;, &#39;red&#39;, &#39;blue&#39;), bty=&#39;n&#39;, lty=2) Code plot(simResults$n, simResults$Zexact_width, type=&#39;b&#39;, ylim=c(0, 2), ylab=&#39;CI width&#39;, xlab=&#39;n&#39;, main=&#39;CI width&#39;) points(simResults$n, simResults$Zapprox_width, type=&#39;b&#39;, col=&#39;red&#39;) points(simResults$n, simResults$T_width, type=&#39;b&#39;, col=&#39;blue&#39;) legend(&#39;bottomright&#39;, legend=c(&#39;Zexact&#39;, &#39;Zapprox&#39;, &#39;T&#39;), col=c(&#39;black&#39;, &#39;red&#39;, &#39;blue&#39;), bty=&#39;n&#39;, lty=2) Code # range(c(CIs$lower, CIs$upper)) Don’t interpret width without having proper coverage. These estimates are noisy, just like the bias/variance simulations. Use more to get better estimates. Rerun these simulations with the gamma errors. With errors from ECT data 3.7 Assumption violations with T-interval Another collaborator I have is evaluating the effect of ECT to treat depression and psychosis symptoms. For this study, my collaborator has reported CGI-I scores The value 4 represents “no change” and he wants to know whether the population mean is likely to be far from 4. What do we do when the assumptions of the T-test aren’t met? Code hist(ect$cgi.i, xlab=&#39;CGI-I&#39;, breaks = c(0:8)-0.5) Code qqnorm(scale(ect$cgi.i)); abline(a=0, b=1) Code ect$cgi.i - mean(ect$cgi.i) ## [1] 0.03125 0.03125 0.03125 0.03125 0.03125 0.03125 -0.96875 0.03125 0.03125 0.03125 0.03125 1.03125 0.03125 0.03125 0.03125 0.03125 ## [17] 2.03125 -0.96875 -0.96875 0.03125 -0.96875 0.03125 0.03125 -0.96875 0.03125 2.03125 0.03125 0.03125 0.03125 0.03125 0.03125 -0.96875 Code t.test(ect$cgi.i) ## ## One Sample t-test ## ## data: ect$cgi.i ## t = 16.027, df = 31, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 1.718218 2.219282 ## sample estimates: ## mean of x ## 1.96875 3.8 Confidence intervals for binomial proportions 3.8.1 CIs using the central limit theorem Remember the CLT: The Central Limit Theorem (Durrett, pg. 124): Let \\(X_1, X_2, \\ldots\\) be iid with \\(\\mathbb{E}X_i = \\mu\\) and \\(\\text{Var}(X_i) = \\sigma^2 \\in (0, \\infty)\\). If \\(\\bar X_n = n^{-1} \\sum_{i=1}^n X_i\\), then \\[ n^{1/2}(\\bar X_n - \\mu)/\\sigma \\to_D X, \\] where \\(X \\sim N(0,1)\\). The CLT allows us to make probabilistic statements about estimators that can be expressed as sums. Since it is an “asymptotic” approximation (infinite sample size), we don’t know how it works in finite samples. Most often, we use simulations to evaluate the finite sample approximation. For homework Problem 1a you will derive this interval, just like we do here. \\[ \\mathbb{P}\\left(\\bar X_n - Z_{0.975}\\times \\sqrt{p(1-p)/n} &lt; p &lt; \\bar X_n + Z_{0.975}\\times \\sqrt{p(1-p)/n}\\right) \\approx 0.95. \\] You will work with this interval for homework 3.8.2 Clopper-Pearson interval for the binomial proportion This is an exact interval (sort of), meaning the probability of coverage is not approximate. To me, it’s not intuitive why this works as a confidence interval (Clopper. Pearson). Conditional on the value \\(X_o \\sim \\text{Binom}(p)\\) (treat it as nonrandom), define \\(p_\\ell(X_o)\\) as the value that satisfies \\[ 0.975 = \\sum_{k=0}^{X_o-1}{ n \\choose k} p_{\\ell}^k(1-p_\\ell)^{n-k} \\] and \\(p_u(X_o)\\) as the value that satisfies \\[ 0.025 = \\sum_{k=0}^{X_o}{ n \\choose k} p_{u}^k(1-p_u)^{n-k} \\] 3.8.3 Picture of this interval Let’s say \\(n=50\\), \\(\\sum_i x_i = 30\\), then choose \\(p\\) so that Code n=50 si = 30 alpha = 0.05 x = 0:n plower = uniroot(function(p) pbinom(si, n, p) - (1-alpha/2), interval=c(0.1, 0.9))$root pupper = uniroot(function(p) pbinom(si, n, p) - alpha/2, interval=c(0.1, 0.9))$root yl = dbinom(x, size=n, prob=plower) yu = dbinom(x, size=n, prob=pupper) plot(x, yl, ylab=&#39;P(sumXi=x)&#39;, xlab=&#39;x&#39;, type=&#39;h&#39;, col=&#39;red&#39;) abline(v=si, col=&#39;black&#39;, lty=2) abline(v=plower*n, col=&#39;red&#39;, lty=2) legend(&#39;topleft&#39;, legend=c(&#39;sumXi&#39;, &#39;lower CI&#39;, &#39;upper CI&#39;), lty=2, col=c(&#39;black&#39;, &#39;red&#39;, &#39;blue&#39;)) Code plot(x, yu, ylab=&#39;P(sumXi=x)&#39;, xlab=&#39;x&#39;, type=&#39;h&#39;, col=&#39;blue&#39;) abline(v=si, col=&#39;black&#39;, lty=2) abline(v=pupper*n, col=&#39;blue&#39;, lty=2) legend(&#39;topleft&#39;, legend=c(&#39;sumXi&#39;, &#39;lower CI&#39;, &#39;upper CI&#39;), lty=2, col=c(&#39;black&#39;, &#39;red&#39;, &#39;blue&#39;)) Code plot(x, yl, ylab=&#39;P(sumXi=x)&#39;, xlab=&#39;x&#39;, type=&#39;h&#39;, col=&#39;red&#39;) points(x, yu, type=&#39;h&#39;, col=&#39;blue&#39;) abline(v=si, col=&#39;black&#39;, lty=2) abline(v=plower*n, col=&#39;red&#39;, lty=2) abline(v=pupper*n, col=&#39;blue&#39;, lty=2) Picture of the interval. 3.8.4 Beta distribution The Beta distribution is a two parameter distribution \\[ f(y; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)}y^{\\alpha-1}(1-y)^{\\beta-1}. \\] Note its similarity to the binomial distribution \\[ p^{\\sum_i x_i}(1- p)^{n-\\sum_i x_i} \\] It turns out that this duality implies that the boundaries for the Clopper-Pearson interval can be expressed in terms of quantiles of the beta distribution \\[ \\left(\\text{qbeta}(\\alpha/2, X, n-X +1), \\text{qbeta}(1-\\alpha/2, X+1, n-X) \\right) \\] 3.9 Extra stuff Code # sample size ns = c(5, 10,50,100,200,500) # correlation rhos = c(0, -0.7) # creating blank output table results = expand.grid(rho=rhos , n=ns) colNames = paste(rep(c(&quot;rhoHat&quot;), each=3), rep(c(&#39;Bias&#39;, &#39;Variance&#39;, &#39;MSE&#39;), 1)) results[, colNames ] = NA for(simInd in 1:nrow(results)){ rho = results[simInd,&#39;rho&#39;] n = results[simInd, &#39;n&#39;] # generate data Sigma = matrix(c(1, rho, rho, 1), nrow=2) sqrtSigma = svd(Sigma) sqrtSigma = sqrtSigma$u %*% diag(sqrt(sqrtSigma$d)) # each column is a draw of x and y rhoHats = replicate(n = 10000, cor(tcrossprod(matrix(rnorm(n*2), ncol=2), sqrtSigma))) # compute bias, variance, MSE of estimators results[simInd, c(&#39;rhoHat Bias&#39;, &#39;rhoHat Variance&#39;)]= c(mean(rhoHats[1,2,])-rho , var(rhoHats[1,2,])) } subres = results[results$rho== -0.7, ] subres2 = results[results$rho== 0,] plot(subres$n, subres$`rhoHat Bias`, xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of rhoHat when rho = -0.7&#39;, type=&#39;b&#39;) Code plot(subres2$n, subres2$`rhoHat Bias`, xlab=&#39;Sample size&#39;, ylab=&#39;Bias&#39;, main=&#39;Bias of rhoHat when rho = 0&#39;, type=&#39;b&#39;) "]]
